{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fashion Search — Indexation DeepFashion In-Shop depuis Colab (GPU)\n\nCe notebook indexe le dataset [DeepFashion In-Shop](https://huggingface.co/datasets/Marqo/deepfashion-inshop) dans Weaviate avec **2 named vectors** :\n- **fashion_clip** (512d) — `patrickjohncyh/fashion-clip`\n- **marqo_clip** (512d) — `Marqo/marqo-fashionCLIP`\n\n**Features :**\n- **Zero-shot tagging** : Fashion CLIP tague chaque image (type, matière, occasion, vibe) pour enrichir les descriptions BM25\n- **Batch GPU encoding** : N images encodées d'un coup par modèle (vs 1 par 1)\n- 2 modèles en parallèle via CUDA streams + ThreadPoolExecutor\n- Combined image + text embeddings (0.5 img + 0.5 txt enrichi par les tags)\n- BM25 search avec boosts (product_name^3, category^2, color^2, description, occasion)\n- Checkpoint / reprise automatique\n- Keep-alive Weaviate (ping toutes les 2 min)\n- **Pas de phase download** — images embarquées dans le dataset HuggingFace (PIL directes)\n\n**Architecture :**\n- **Colab (GPU)** : encode les images en batch avec les 2 modèles → pousse les vecteurs dans Weaviate\n- **GCP (CPU)** : Weaviate + app FastAPI pour servir les recherches\n\n**Dataset :** 52.6k images produit studio (224×224), ~7-8k produits uniques après dédoublonnage.\n\n**Prérequis :**\n- Runtime GPU activé (Runtime → Change runtime type → T4 GPU)\n- Weaviate accessible sur ta VM GCP (port 8080 ouvert dans le firewall)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install deps (sans toucher au torch pre-installe sur Colab)\n!pip install -q weaviate-client>=4.0 transformers Pillow datasets requests tqdm timm ftfy regex\n!pip install -q --no-deps open-clip-torch"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Renseigne l'IP externe de ta VM GCP où tourne Weaviate.\n",
    "\n",
    "**Important** : le port `8080` (HTTP) et `50051` (gRPC) doivent être ouverts dans le firewall GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- CONFIGURATION ---\nGCP_EXTERNAL_IP = \"\"   # ex: \"34.56.78.90\"\n\nWEAVIATE_HTTP_PORT = 8080\nWEAVIATE_GRPC_PORT = 50051\n\nCOLLECTION_NAME = \"FashionCollection\"\n\n# Dataset\nDATASET_NAME = \"Marqo/deepfashion-inshop\"\nMAX_VIEWS_PER_PRODUCT = 1          # 1 seule vue par produit (front de préférence)\n\n# Modèles (2 modèles spécialisés fashion)\nMODEL_FASHION_CLIP = \"patrickjohncyh/fashion-clip\"\nMODEL_MARQO_CLIP = \"Marqo/marqo-fashionCLIP\"\n\n# Named vector keys\nVECTOR_FASHION_CLIP = \"fashion_clip\"\nVECTOR_MARQO_CLIP = \"marqo_clip\"\nALL_VECTOR_NAMES = [VECTOR_FASHION_CLIP, VECTOR_MARQO_CLIP]\n\n# --- Tuning ---\nMAX_ITEMS = None              # tout le dataset\nWEAVIATE_BATCH_SIZE = 500     # objets par flush vers Weaviate\nENCODE_BATCH_SIZE = 128       # images par batch GPU (128 pour A100, 32 pour T4)\nTHUMBNAIL_SIZE = 150\nIMAGE_WEIGHT = 0.5            # 50/50 image et texte\nRECREATE = False              # False = reprise, True = tout recréer\n\n# Marques fictives\nBRANDS = [\n    # Sportswear & Streetwear\n    \"Nike\", \"Adidas\", \"Puma\", \"New Balance\", \"The North Face\",\n    # Premium & Designers\n    \"Calvin Klein\", \"Tommy Hilfiger\", \"Polo Ralph Lauren\", \"BOSS\", \"Armani Exchange\",\n    # Casual & Denim\n    \"Levi's\", \"Topshop\", \"Dr Martens\", \"Carhartt WIP\", \"Pull&Bear\",\n    # Accessible & Trendy\n    \"Mango\", \"Stradivarius\", \"Bershka\", \"Superdry\", \"AllSaints\",\n]\n\n# Prix aléatoires par catégorie (min, max en £)\nPRICE_RANGES = {\n    \"denim\": (30, 90),\n    \"jackets\": (50, 200),\n    \"tops\": (15, 60),\n    \"sweaters\": (30, 100),\n    \"shorts\": (20, 60),\n    \"pants\": (30, 90),\n    \"skirts\": (25, 80),\n    \"dresses\": (35, 150),\n    \"rompers\": (30, 80),\n    \"swimwear\": (20, 70),\n    \"shoes\": (40, 180),\n    \"bags\": (30, 250),\n    \"accessories\": (10, 80),\n    \"intimates\": (10, 50),\n    \"jumpsuits\": (40, 120),\n    \"suiting\": (80, 250),\n}\n\n# Checkpoint (reprise automatique)\nCHECKPOINT_FILE = \"/content/index_checkpoint.json\"\nKEEPALIVE_INTERVAL = 120      # secondes"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vérification GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\n\nif torch.cuda.is_available():\n    device = \"cuda\"\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nelse:\n    device = \"cpu\"\n    print(\"Pas de GPU, l'indexation sera lente.\")\n\nprint(f\"Device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Chargement des 2 modèles"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import CLIPModel, CLIPProcessor\nimport open_clip\n\n# Fashion CLIP (512d) — HuggingFace format\nprint(f\"Chargement de {MODEL_FASHION_CLIP}...\")\nfc_model = CLIPModel.from_pretrained(MODEL_FASHION_CLIP).to(device).eval()\nfc_processor = CLIPProcessor.from_pretrained(MODEL_FASHION_CLIP)\n\n# Marqo FashionCLIP (512d) — OpenCLIP format\nprint(f\"Chargement de {MODEL_MARQO_CLIP}...\")\nmq_model, _, mq_preprocess = open_clip.create_model_and_transforms(\n    f\"hf-hub:{MODEL_MARQO_CLIP}\", device=device\n)\nmq_tokenizer = open_clip.get_tokenizer(f\"hf-hub:{MODEL_MARQO_CLIP}\")\nmq_model = mq_model.eval()\n\n# Dict unifie : (type, model, processor/preprocess, tokenizer, max_text_tokens)\nMODELS = {\n    VECTOR_FASHION_CLIP: (\"hf\", fc_model, fc_processor, None, 77),\n    VECTOR_MARQO_CLIP: (\"openclip\", mq_model, mq_preprocess, mq_tokenizer, 77),\n}\n\nif device == \"cuda\":\n    vram_used = torch.cuda.memory_allocated() / 1024**3\n    vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"\\n2 modeles charges sur {device} — VRAM: {vram_used:.1f}/{vram_total:.0f} GB\")\nelse:\n    print(f\"\\n2 modeles charges sur {device}.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Connexion a Weaviate sur GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import weaviate\nimport json\nimport os\nimport threading\nfrom datetime import datetime, timezone\nfrom weaviate.classes.config import Configure, DataType, Property, VectorDistances\n\nassert GCP_EXTERNAL_IP, \"Renseigne GCP_EXTERNAL_IP dans la cellule de configuration.\"\n\nclient = weaviate.connect_to_custom(\n    http_host=GCP_EXTERNAL_IP,\n    http_port=WEAVIATE_HTTP_PORT,\n    http_secure=False,\n    grpc_host=GCP_EXTERNAL_IP,\n    grpc_port=WEAVIATE_GRPC_PORT,\n    grpc_secure=False,\n)\n\nprint(f\"Connecte a Weaviate sur {GCP_EXTERNAL_IP}\" if client.is_ready() else \"Echec de connexion\")\n\n# --- Keep-alive (ping toutes les 2 min, auto-reconnect) ---\n_keepalive_stop = threading.Event()\n\ndef start_keepalive(wv_client, interval=KEEPALIVE_INTERVAL):\n    def _run():\n        while not _keepalive_stop.wait(interval):\n            try:\n                if not wv_client.is_ready():\n                    raise ConnectionError(\"not ready\")\n            except Exception as e:\n                print(f\"\\n[keep-alive] Reconnexion... ({e})\")\n                try:\n                    wv_client.close()\n                except Exception:\n                    pass\n                wv_client.connect_to_custom(\n                    http_host=GCP_EXTERNAL_IP,\n                    http_port=WEAVIATE_HTTP_PORT,\n                    http_secure=False,\n                    grpc_host=GCP_EXTERNAL_IP,\n                    grpc_port=WEAVIATE_GRPC_PORT,\n                    grpc_secure=False,\n                )\n    t = threading.Thread(target=_run, daemon=True)\n    t.start()\n    return t\n\nstart_keepalive(client)\nprint(\"Keep-alive demarre (ping toutes les 2 min)\")\n\n# --- Checkpoint helpers ---\ndef load_checkpoint():\n    try:\n        if os.path.exists(CHECKPOINT_FILE):\n            with open(CHECKPOINT_FILE) as f:\n                return json.load(f)\n    except Exception:\n        pass\n    return {\"last_index\": -1, \"indexed_count\": 0, \"errors\": 0}\n\ndef save_checkpoint(last_index, indexed_count, errors):\n    with open(CHECKPOINT_FILE, \"w\") as f:\n        json.dump({\"last_index\": last_index, \"indexed_count\": indexed_count,\n                    \"errors\": errors, \"timestamp\": datetime.now(timezone.utc).isoformat()}, f)\n\ndef clear_checkpoint():\n    if os.path.exists(CHECKPOINT_FILE):\n        os.remove(CHECKPOINT_FILE)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creation du schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_schema(wv_client, collection_name, vector_names=None):\n    \"\"\"Crée la collection avec named vectors.\"\"\"\n    if vector_names is None:\n        vector_names = ALL_VECTOR_NAMES\n\n    named_vectors = [\n        Configure.NamedVectors.none(\n            name=name,\n            vector_index_config=Configure.VectorIndex.hnsw(\n                distance_metric=VectorDistances.COSINE\n            ),\n        )\n        for name in vector_names\n    ]\n\n    wv_client.collections.create(\n        name=collection_name,\n        properties=[\n            Property(name=\"filename\", data_type=DataType.TEXT),\n            Property(name=\"path\", data_type=DataType.TEXT),\n            Property(name=\"thumbnail_base64\", data_type=DataType.TEXT),\n            Property(name=\"width\", data_type=DataType.INT),\n            Property(name=\"height\", data_type=DataType.INT),\n            Property(name=\"indexed_at\", data_type=DataType.TEXT),\n            Property(name=\"product_id\", data_type=DataType.TEXT),\n            Property(name=\"product_name\", data_type=DataType.TEXT),\n            Property(name=\"category\", data_type=DataType.TEXT),\n            Property(name=\"color\", data_type=DataType.TEXT),\n            Property(name=\"size\", data_type=DataType.TEXT),\n            Property(name=\"price\", data_type=DataType.NUMBER),\n            Property(name=\"brand\", data_type=DataType.TEXT),\n            Property(name=\"product_url\", data_type=DataType.TEXT),\n            Property(name=\"description\", data_type=DataType.TEXT),\n            Property(name=\"image_index\", data_type=DataType.INT),\n            Property(name=\"gender\", data_type=DataType.TEXT),\n            Property(name=\"occasion\", data_type=DataType.TEXT),\n        ],\n        vectorizer_config=named_vectors,\n    )\n    print(f\"Collection '{collection_name}' creee avec vecteurs: {vector_names}\")\n\n\nif RECREATE:\n    if client.collections.exists(COLLECTION_NAME):\n        client.collections.delete(COLLECTION_NAME)\n        print(f\"Collection '{COLLECTION_NAME}' supprimee.\")\n    create_schema(client, COLLECTION_NAME)\n    clear_checkpoint()\nelif not client.collections.exists(COLLECTION_NAME):\n    create_schema(client, COLLECTION_NAME)\n    clear_checkpoint()\nelse:\n    print(f\"Collection '{COLLECTION_NAME}' existe deja, on garde les donnees.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Chargement du dataset DeepFashion In-Shop"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from datasets import load_dataset\nfrom collections import Counter\nimport random\n\nprint(f\"Chargement du dataset {DATASET_NAME}...\")\nds = load_dataset(DATASET_NAME, split=\"data\")\nprint(f\"Dataset chargé: {len(ds)} images totales\")\n\n# --- Dédoublonnage par produit (garder 1 vue, front en priorité) ---\n# item_ID format: \"MEN_Denim_id_00000080_01_1_front\"\n# product_id = tout sauf les 2 derniers segments (view_number + view_type)\nVIEW_PRIORITY = {\"front\": 0, \"full\": 1, \"additional\": 2, \"side\": 3, \"back\": 4}\n\nproduct_views = {}  # product_id -> (priority, index_in_dataset)\n\nfor i in range(len(ds)):\n    item_id = ds[i][\"item_ID\"]\n    parts = item_id.rsplit(\"_\", 2)  # split from right, max 2 splits\n    product_id = parts[0]           # everything before last 2 segments\n    view_type = parts[-1] if len(parts) > 1 else \"unknown\"\n    priority = VIEW_PRIORITY.get(view_type, 99)\n\n    if product_id not in product_views or priority < product_views[product_id][0]:\n        product_views[product_id] = (priority, i)\n\n# Build deduplicated list of dataset indices\nselected_indices = [idx for _, idx in product_views.values()]\nselected_indices.sort()\n\nif MAX_ITEMS:\n    selected_indices = selected_indices[:MAX_ITEMS]\n\nprint(f\"Produits uniques: {len(selected_indices)} (sur {len(ds)} images)\")\nprint(f\"Vue sélectionnée par priorité: front > full > additional > side > back\")\n\n# Stats par catégorie\ncat_counts = Counter(ds[i][\"category2\"] for i in selected_indices)\nprint(f\"\\nCatégories ({len(cat_counts)}):\")\nfor cat, count in cat_counts.most_common():\n    print(f\"  {cat}: {count}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import base64\nimport io\nfrom contextlib import nullcontext\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport numpy as np\nfrom PIL import Image\n\n\n# =====================================================================\n# Encode functions (single image + batch)\n# =====================================================================\n\ndef _to_tensor(feat):\n    \"\"\"Extract tensor from model output (handles both raw tensors and BaseModelOutput).\"\"\"\n    if isinstance(feat, torch.Tensor):\n        return feat\n    if hasattr(feat, \"pooler_output\") and feat.pooler_output is not None:\n        return feat.pooler_output\n    if hasattr(feat, \"last_hidden_state\"):\n        return feat.last_hidden_state[:, 0, :]\n    raise ValueError(f\"Cannot extract tensor from {type(feat)}\")\n\n\ndef _encode_image(model_type, model, processor, img):\n    \"\"\"Encode image -> normalized numpy vector.\"\"\"\n    if model_type == \"openclip\":\n        image_tensor = processor(img).unsqueeze(0).to(device)\n        feat = model.encode_image(image_tensor)\n    else:\n        inputs = processor(images=img, return_tensors=\"pt\")\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        feat = model.get_image_features(**inputs)\n    feat = _to_tensor(feat)\n    feat = feat / feat.norm(p=2, dim=-1, keepdim=True)\n    return feat.detach().cpu().float().numpy().flatten()\n\n\ndef _encode_text(model_type, model, processor, tokenizer, text, max_len=77):\n    \"\"\"Encode text -> normalized numpy vector.\"\"\"\n    if model_type == \"openclip\":\n        tokens = tokenizer([text]).to(device)\n        feat = model.encode_text(tokens)\n    else:\n        tok = getattr(processor, \"tokenizer\", processor)\n        inputs = tok(\n            [text], return_tensors=\"pt\", padding=True,\n            truncation=True, max_length=max_len,\n        )\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        feat = model.get_text_features(**inputs)\n    feat = _to_tensor(feat)\n    feat = feat / feat.norm(p=2, dim=-1, keepdim=True)\n    return feat.detach().cpu().float().numpy().flatten()\n\n\ndef _encode_images_batch(model_type, model, processor, images):\n    \"\"\"Encode a batch of images -> normalized numpy vectors (N, D).\"\"\"\n    if model_type == \"openclip\":\n        batch = torch.stack([processor(img) for img in images]).to(device)\n        feat = model.encode_image(batch)\n    else:\n        inputs = processor(images=images, return_tensors=\"pt\")\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        feat = model.get_image_features(**inputs)\n    feat = _to_tensor(feat)\n    feat = feat / feat.norm(p=2, dim=-1, keepdim=True)\n    return feat.detach().cpu().float().numpy()\n\n\ndef _encode_texts_batch(model_type, model, processor, tokenizer, texts, max_len=77):\n    \"\"\"Encode a batch of texts -> normalized numpy vectors (N, D).\"\"\"\n    if model_type == \"openclip\":\n        tokens = tokenizer(texts).to(device)\n        feat = model.encode_text(tokens)\n    else:\n        tok = getattr(processor, \"tokenizer\", processor)\n        inputs = tok(\n            texts, return_tensors=\"pt\", padding=True,\n            truncation=True, max_length=max_len,\n        )\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        feat = model.get_text_features(**inputs)\n    feat = _to_tensor(feat)\n    feat = feat / feat.norm(p=2, dim=-1, keepdim=True)\n    return feat.detach().cpu().float().numpy()\n\n\n# =====================================================================\n# Zero-shot tagging with Fashion CLIP\n# =====================================================================\n\nLABEL_TAXONOMY = {\n    \"garment\": [\n        \"t-shirt\", \"blouse\", \"shirt\", \"crop top\", \"tank top\", \"camisole\",\n        \"polo shirt\", \"henley\", \"tunic\", \"bodysuit\",\n        \"jeans\", \"trousers\", \"shorts\", \"leggings\", \"chinos\", \"joggers\",\n        \"wide leg pants\", \"cargo pants\", \"culottes\",\n        \"mini dress\", \"midi dress\", \"maxi dress\", \"shirt dress\", \"wrap dress\",\n        \"cocktail dress\", \"evening gown\", \"sundress\",\n        \"jumpsuit\", \"romper\", \"playsuit\",\n        \"blazer\", \"leather jacket\", \"denim jacket\", \"bomber jacket\",\n        \"puffer jacket\", \"trench coat\", \"overcoat\", \"parka\",\n        \"cardigan\", \"hoodie\", \"sweater\", \"pullover\", \"vest\",\n        \"mini skirt\", \"midi skirt\", \"maxi skirt\", \"pleated skirt\", \"pencil skirt\",\n        \"bikini\", \"swimsuit\", \"bra\", \"lingerie\",\n        \"handbag\", \"backpack\", \"tote bag\", \"clutch bag\", \"crossbody bag\",\n        \"sneakers\", \"boots\", \"ankle boots\", \"heels\", \"sandals\", \"loafers\",\n        \"suit\", \"tuxedo\",\n    ],\n    \"description\": [\n        # Matières\n        \"leather\", \"faux leather\", \"suede\", \"denim\", \"cotton\", \"linen\",\n        \"silk\", \"satin\", \"chiffon\", \"velvet\", \"wool\", \"cashmere\",\n        \"knit\", \"mesh\", \"lace\", \"tulle\", \"sequin\", \"metallic\",\n        \"corduroy\", \"tweed\", \"jersey\", \"fleece\",\n        # Coupes & détails\n        \"oversized\", \"slim fit\", \"fitted\", \"relaxed fit\", \"cropped\",\n        \"high waisted\", \"flared\", \"skinny\", \"pleated\", \"ruffled\",\n        \"embroidered\", \"belted\", \"hooded\", \"collared\",\n        \"v-neck\", \"crew neck\", \"turtleneck\", \"off-shoulder\", \"strapless\",\n        \"sleeveless\", \"long sleeve\", \"short sleeve\",\n        # Motifs\n        \"floral print\", \"striped\", \"plaid\", \"polka dot\", \"animal print\",\n        # Style\n        \"minimalist\", \"vintage\", \"bohemian\", \"elegant\", \"edgy\", \"sporty\",\n    ],\n    \"occasion\": [\n        # Formel / soirée\n        \"evening party\", \"cocktail party\", \"formal dinner\", \"gala event\",\n        \"black tie event\", \"wedding guest\", \"prom night\", \"awards ceremony\",\n        \"elegant night out\", \"fancy restaurant\",\n        # Bureau / professionnel\n        \"office work\", \"business meeting\", \"job interview\", \"conference\",\n        \"professional setting\",\n        # Casual / quotidien\n        \"casual everyday\", \"weekend outing\", \"brunch with friends\",\n        \"shopping trip\", \"coffee date\", \"running errands\",\n        # Sport / outdoor\n        \"gym workout\", \"outdoor hiking\", \"yoga session\", \"running jogging\",\n        \"athleisure streetwear\",\n        # Vacances / loisirs\n        \"beach vacation\", \"summer festival\", \"travel\", \"resort wear\",\n        \"garden party\", \"picnic outdoor\",\n        # Date / romantique\n        \"date night\", \"romantic dinner\",\n    ],\n    \"vibe\": [\n        \"elegant sophisticated\", \"edgy bold\", \"romantic feminine\",\n        \"sporty athletic\", \"classic timeless\", \"modern minimalist\",\n        \"bohemian free-spirited\", \"glamorous luxurious\",\n        \"casual relaxed\", \"professional polished\", \"playful fun\",\n        \"streetwear cool\", \"chic refined\", \"grunge rebellious\",\n    ],\n    \"gender\": [\n        \"male model\",\n        \"female model\",\n    ],\n}\n\nTAG_TOP_K = {\n    \"garment\": 3,\n    \"description\": 4,\n    \"occasion\": 5,\n    \"vibe\": 2,\n    \"gender\": 1,\n}\nTAG_MIN_SIMILARITY = 0.15\n\nLABEL_PROMPTS = {\n    \"garment\": \"a photo of {}\",\n    \"description\": \"a photo of {} clothing\",\n    \"occasion\": \"outfit suitable for {}\",\n    \"vibe\": \"a {} fashion style\",\n    \"gender\": \"a photo of a {}\",\n}\n\nGARMENT_OCCASION_MAP = {\n    # Robes\n    \"cocktail dress\": \"evening party, cocktail party, formal dinner, date night, elegant night out\",\n    \"evening gown\": \"gala event, black tie event, awards ceremony, prom night, elegant night out\",\n    \"mini dress\": \"evening party, date night, brunch with friends, cocktail party, weekend outing\",\n    \"midi dress\": \"office work, wedding guest, brunch with friends, date night, garden party\",\n    \"maxi dress\": \"beach vacation, garden party, wedding guest, summer festival, resort wear\",\n    \"shirt dress\": \"office work, brunch with friends, casual everyday, shopping trip, conference\",\n    \"wrap dress\": \"office work, date night, wedding guest, brunch with friends, professional setting\",\n    \"sundress\": \"beach vacation, summer festival, garden party, picnic outdoor, weekend outing\",\n    # Hauts\n    \"t-shirt\": \"casual everyday, weekend outing, running errands, shopping trip, travel\",\n    \"blouse\": \"office work, business meeting, brunch with friends, date night, professional setting\",\n    \"shirt\": \"office work, business meeting, casual everyday, conference, job interview\",\n    \"crop top\": \"summer festival, beach vacation, casual everyday, weekend outing, date night\",\n    \"tank top\": \"gym workout, beach vacation, casual everyday, summer festival, running jogging\",\n    \"camisole\": \"date night, evening party, casual everyday, romantic dinner, elegant night out\",\n    \"polo shirt\": \"casual everyday, weekend outing, office work, brunch with friends, travel\",\n    \"henley\": \"casual everyday, weekend outing, coffee date, running errands, travel\",\n    \"tunic\": \"casual everyday, beach vacation, brunch with friends, weekend outing, resort wear\",\n    \"bodysuit\": \"evening party, date night, cocktail party, casual everyday, elegant night out\",\n    # Pantalons\n    \"jeans\": \"casual everyday, weekend outing, shopping trip, brunch with friends, coffee date\",\n    \"trousers\": \"office work, business meeting, professional setting, conference, job interview\",\n    \"shorts\": \"beach vacation, casual everyday, summer festival, weekend outing, gym workout\",\n    \"leggings\": \"gym workout, yoga session, running jogging, athleisure streetwear, casual everyday\",\n    \"chinos\": \"office work, casual everyday, brunch with friends, weekend outing, business meeting\",\n    \"joggers\": \"casual everyday, athleisure streetwear, running errands, gym workout, travel\",\n    \"wide leg pants\": \"office work, brunch with friends, casual everyday, weekend outing, travel\",\n    \"cargo pants\": \"casual everyday, outdoor hiking, travel, weekend outing, running errands\",\n    \"culottes\": \"office work, brunch with friends, casual everyday, weekend outing, shopping trip\",\n    # Combinaisons\n    \"jumpsuit\": \"evening party, date night, wedding guest, brunch with friends, cocktail party\",\n    \"romper\": \"beach vacation, summer festival, casual everyday, weekend outing, garden party\",\n    \"playsuit\": \"beach vacation, summer festival, weekend outing, garden party, casual everyday\",\n    # Vestes & manteaux\n    \"blazer\": \"office work, business meeting, job interview, conference, professional setting\",\n    \"leather jacket\": \"date night, evening party, casual everyday, weekend outing, concert\",\n    \"denim jacket\": \"casual everyday, weekend outing, shopping trip, brunch with friends, travel\",\n    \"bomber jacket\": \"casual everyday, athleisure streetwear, weekend outing, travel, shopping trip\",\n    \"puffer jacket\": \"outdoor hiking, casual everyday, travel, running errands, weekend outing\",\n    \"trench coat\": \"office work, business meeting, casual everyday, professional setting, travel\",\n    \"overcoat\": \"office work, formal dinner, business meeting, professional setting, elegant night out\",\n    \"parka\": \"outdoor hiking, casual everyday, travel, running errands, weekend outing\",\n    # Maille\n    \"cardigan\": \"office work, casual everyday, weekend outing, coffee date, brunch with friends\",\n    \"hoodie\": \"casual everyday, weekend outing, athleisure streetwear, running errands, gym workout\",\n    \"sweater\": \"casual everyday, weekend outing, office work, coffee date, travel\",\n    \"pullover\": \"casual everyday, weekend outing, office work, coffee date, travel\",\n    \"vest\": \"casual everyday, outdoor hiking, office work, weekend outing, travel\",\n    # Jupes\n    \"mini skirt\": \"evening party, date night, weekend outing, brunch with friends, casual everyday\",\n    \"midi skirt\": \"office work, brunch with friends, wedding guest, professional setting, date night\",\n    \"maxi skirt\": \"beach vacation, casual everyday, wedding guest, garden party, resort wear\",\n    \"pleated skirt\": \"office work, brunch with friends, wedding guest, casual everyday, date night\",\n    \"pencil skirt\": \"office work, business meeting, professional setting, conference, job interview\",\n    # Swimwear & lingerie\n    \"bikini\": \"beach vacation, resort wear, summer festival, picnic outdoor\",\n    \"swimsuit\": \"beach vacation, resort wear, summer festival\",\n    \"bra\": \"casual everyday\",\n    \"lingerie\": \"romantic dinner, date night\",\n    # Sacs\n    \"handbag\": \"office work, shopping trip, casual everyday, brunch with friends, date night\",\n    \"backpack\": \"travel, casual everyday, outdoor hiking, running errands, weekend outing\",\n    \"tote bag\": \"office work, shopping trip, beach vacation, casual everyday, brunch with friends\",\n    \"clutch bag\": \"evening party, cocktail party, formal dinner, date night, gala event\",\n    \"crossbody bag\": \"casual everyday, travel, shopping trip, weekend outing, running errands\",\n    # Chaussures\n    \"sneakers\": \"casual everyday, athleisure streetwear, weekend outing, travel, running errands\",\n    \"boots\": \"casual everyday, weekend outing, outdoor hiking, date night, travel\",\n    \"ankle boots\": \"casual everyday, date night, weekend outing, office work, brunch with friends\",\n    \"heels\": \"evening party, cocktail party, formal dinner, date night, office work\",\n    \"sandals\": \"beach vacation, casual everyday, summer festival, garden party, resort wear\",\n    \"loafers\": \"office work, casual everyday, brunch with friends, professional setting, weekend outing\",\n    # Costumes\n    \"suit\": \"business meeting, job interview, formal dinner, conference, professional setting\",\n    \"tuxedo\": \"gala event, black tie event, awards ceremony, formal dinner, prom night\",\n}\n\n\ndef occasions_from_garment_tags(garment_tags):\n    \"\"\"Derive occasions from garment tags using domain mapping.\"\"\"\n    scores = {}\n    for tag in garment_tags:\n        occs = GARMENT_OCCASION_MAP.get(tag, \"\")\n        if occs:\n            for i, occ in enumerate(occs.split(\", \")):\n                scores[occ] = scores.get(occ, 0) + 1.0 / (i + 1)\n    return sorted(scores, key=scores.get, reverse=True)[:5]\n\n\ndef precompute_label_embeddings():\n    \"\"\"Pre-encode all taxonomy labels with Fashion CLIP (once).\"\"\"\n    label_embeddings = {}\n    fc_type, fc_mdl, fc_proc, fc_tok, fc_max = MODELS[VECTOR_FASHION_CLIP]\n\n    for category, labels in LABEL_TAXONOMY.items():\n        prompt_tpl = LABEL_PROMPTS.get(category, \"a photo of {}\")\n        prompts = [prompt_tpl.format(label) for label in labels]\n        vecs = _encode_texts_batch(fc_type, fc_mdl, fc_proc, fc_tok, prompts, fc_max)\n        label_embeddings[category] = {\n            \"labels\": labels,\n            \"vectors\": vecs,  # (M, 512) normalized\n        }\n        print(f\"  {category}: {len(labels)} labels encoded\")\n\n    return label_embeddings\n\n\ndef tag_images_batch(fc_image_vecs, label_embeddings):\n    \"\"\"Zero-shot tag a batch of images using pre-computed label embeddings.\n\n    Args:\n        fc_image_vecs: (N, 512) normalized Fashion CLIP image vectors\n        label_embeddings: dict from precompute_label_embeddings()\n\n    Returns:\n        list of N dicts, each: {category: [top-K label strings]}\n    \"\"\"\n    n = fc_image_vecs.shape[0]\n    all_tags = [{} for _ in range(n)]\n\n    for category, data in label_embeddings.items():\n        lab_vecs = data[\"vectors\"]  # (M, 512)\n        labels = data[\"labels\"]\n        top_k = TAG_TOP_K.get(category, 3)\n\n        # Cosine similarity: (N, 512) @ (512, M) -> (N, M)\n        sims = fc_image_vecs @ lab_vecs.T\n\n        for i in range(n):\n            row = sims[i]\n            top_indices = np.argsort(row)[::-1][:top_k]\n            tags = [labels[j] for j in top_indices if row[j] >= TAG_MIN_SIMILARITY]\n            all_tags[i][category] = tags\n\n    return all_tags\n\n\ndef build_enriched_description(tags, original_desc=\"\"):\n    \"\"\"Build enriched description from zero-shot tags + original description.\n\n    Returns:\n        (description, occasion, gender) tuple\n    \"\"\"\n    # Garment mapping occasions (primary, precise)\n    garment_occasions = occasions_from_garment_tags(tags.get(\"garment\", []))\n    # Zero-shot occasions (secondary, visual cues)\n    clip_occasions = tags.get(\"occasion\", [])\n    # Merge: garment mapping first, then unique CLIP additions\n    merged = list(garment_occasions)\n    for occ in clip_occasions:\n        if occ not in merged:\n            merged.append(occ)\n    final_occasions = merged[:5]\n\n    # Build description\n    parts = []\n    for cat in [\"garment\", \"description\"]:\n        if tags.get(cat):\n            parts.append(\", \".join(tags[cat]))\n    if final_occasions:\n        parts.append(\", \".join(final_occasions))\n    if tags.get(\"vibe\"):\n        parts.append(\", \".join(tags[\"vibe\"]))\n\n    # Map gender tag to BM25-friendly text\n    gender_tags = tags.get(\"gender\", [])\n    gender = None\n    if gender_tags:\n        g = gender_tags[0]\n        if \"male\" in g and \"female\" not in g:\n            gender = \"men\"\n            parts.append(\"men's clothing\")\n        elif \"female\" in g:\n            gender = \"women\"\n            parts.append(\"women's clothing\")\n\n    tag_text = \" | \".join(parts)\n    description = f\"{tag_text} | {original_desc.strip()}\" if original_desc and original_desc.strip() else tag_text\n    occasion = \", \".join(final_occasions)\n\n    return description, occasion, gender\n\n\n# Pre-compute label embeddings\nprint(\"Pre-computing label embeddings with Fashion CLIP...\")\n_label_embeddings = precompute_label_embeddings()\ntotal_labels = sum(len(v[\"labels\"]) for v in _label_embeddings.values())\nprint(f\"Done: {total_labels} labels across {len(_label_embeddings)} categories\")\n\n\n# =====================================================================\n# Thumbnail helper\n# =====================================================================\n\ndef make_thumbnail_b64(image, size=THUMBNAIL_SIZE):\n    img = image.copy()\n    img.thumbnail((size, size))\n    if img.mode in (\"RGBA\", \"P\"):\n        img = img.convert(\"RGB\")\n    buf = io.BytesIO()\n    img.save(buf, format=\"JPEG\", quality=85)\n    return base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n\n\n# =====================================================================\n# Combined encoding (image + text, multi-model)\n# =====================================================================\n\n# Pre-create one CUDA stream per model for true GPU parallelism\n_model_streams = {}\nif device == \"cuda\":\n    for name in MODELS:\n        _model_streams[name] = torch.cuda.Stream()\n    print(f\"Created {len(_model_streams)} CUDA streams for parallel encoding\")\n\n\ndef encode_combined_batch(images, texts, image_weight=IMAGE_WEIGHT,\n                          fc_image_vecs_precomputed=None):\n    \"\"\"Encode N images+texts avec les 2 modeles en parallele (batch GPU + CUDA streams).\n\n    Args:\n        images: list of PIL images\n        texts: list of text descriptions\n        image_weight: weight for image vs text (default 0.5)\n        fc_image_vecs_precomputed: optional (N, 512) Fashion CLIP image vecs\n            already computed (reused from tagging step to avoid double encoding)\n    \"\"\"\n    n = len(images)\n    imgs = [img.convert(\"RGB\") for img in images]\n    has_texts = [bool(t and t.strip()) for t in texts]\n    text_indices = [i for i in range(n) if has_texts[i]]\n    batch_texts = [texts[i] for i in text_indices] if text_indices else []\n\n    def encode_model(name, model_tuple):\n        model_type, mdl, processor, tokenizer, max_len = model_tuple\n        stream = _model_streams.get(name)\n        stream_ctx = torch.cuda.stream(stream) if stream else nullcontext()\n        with torch.no_grad(), stream_ctx:\n            # Reuse precomputed Fashion CLIP image vecs if available\n            if name == VECTOR_FASHION_CLIP and fc_image_vecs_precomputed is not None:\n                img_vecs = fc_image_vecs_precomputed\n            else:\n                img_vecs = _encode_images_batch(model_type, mdl, processor, imgs)\n            if text_indices:\n                txt_vecs = _encode_texts_batch(model_type, mdl, processor, tokenizer, batch_texts, max_len)\n                combined = img_vecs.copy()\n                for j, idx in enumerate(text_indices):\n                    c = image_weight * img_vecs[idx] + (1 - image_weight) * txt_vecs[j]\n                    norm_val = np.linalg.norm(c)\n                    if norm_val > 0:\n                        c = c / norm_val\n                    combined[idx] = c\n                result = combined\n            else:\n                result = img_vecs\n        if stream:\n            stream.synchronize()\n        return name, result\n\n    with ThreadPoolExecutor(max_workers=2) as executor:\n        futures = [executor.submit(encode_model, nm, t) for nm, t in MODELS.items()]\n        all_vectors = {f.result()[0]: f.result()[1] for f in futures}\n\n    return [{name: all_vectors[name][i] for name in all_vectors} for i in range(n)]\n\n\n# Quick sanity check (single image)\n_test_img = Image.new(\"RGB\", (64, 64), \"red\")\n_test_vecs = _encode_images_batch(*MODELS[VECTOR_FASHION_CLIP][:3], [_test_img])\nprint(f\"  Fashion CLIP image vec: shape={_test_vecs.shape}\")\n\n# Sanity check: zero-shot tagging\n_test_tags = tag_images_batch(_test_vecs, _label_embeddings)\nprint(f\"  Tags for red 64x64 image: {_test_tags[0]}\")\n\n# Sanity check (batch of 4)\n_test_imgs = [Image.new(\"RGB\", (64, 64), c) for c in [\"red\", \"blue\", \"green\", \"black\"]]\n_test_texts = [\"red shirt\", \"blue jeans\", \"green jacket\", \"black shoes\"]\n_test_batch = encode_combined_batch(_test_imgs, _test_texts)\nprint(f\"  Batch: {len(_test_batch)} items, keys={list(_test_batch[0].keys())}\")\nfor k in _test_batch[0]:\n    print(f\"    {k}: shape={_test_batch[0][k].shape}\")\nprint(\"OK\")"
  },
  {
   "cell_type": "code",
   "source": "## Benchmark : batch encoding speed\nimport time\n\nN_BENCH = 32  # nombre d'images pour le benchmark\n_bench_imgs = [Image.new(\"RGB\", (384, 384), c) for c in [\"blue\", \"red\", \"green\", \"black\"] * (N_BENCH // 4)]\n_bench_texts = [f\"color {i} denim jacket casual style\" for i in range(N_BENCH)]\n\n# Warmup GPU\nfor _ in range(3):\n    encode_combined_batch([_bench_imgs[0]], [_bench_texts[0]])\nif device == \"cuda\":\n    torch.cuda.synchronize()\n\n# --- Batch (all images at once) ---\nif device == \"cuda\":\n    torch.cuda.synchronize()\nt0 = time.perf_counter()\nencode_combined_batch(_bench_imgs, _bench_texts)\nif device == \"cuda\":\n    torch.cuda.synchronize()\nbatch_time = time.perf_counter() - t0\n\nprint(f\"Benchmark sur {N_BENCH} images ({device}):\")\nprint(f\"  Batch (x{N_BENCH})  : {batch_time:.2f}s ({N_BENCH/batch_time:.1f} img/s) — {batch_time/N_BENCH*1000:.0f}ms/img\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Indexation (GPU batch encoding + zero-shot tagging → Weaviate sur GCP)\n\nPipeline : images PIL directes → Fashion CLIP image encode → zero-shot tagging → enriched descriptions → batch encode (2 modèles) → push Weaviate.\n\n**Optimisation** : Fashion CLIP image vecs calculés une seule fois, réutilisés pour le tagging ET la vectorisation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nimport random\nfrom datetime import datetime, timezone\nfrom tqdm.auto import tqdm\nfrom PIL import Image\n\ncollection = client.collections.get(COLLECTION_NAME)\n\n# --- Checkpoint & reprise ---\ncheckpoint = load_checkpoint()\nstart_idx = 0\nif RECREATE:\n    start_idx = 0\nelif checkpoint[\"last_index\"] >= 0:\n    start_idx = checkpoint[\"last_index\"] + 1\n    print(f\"Reprise depuis l'index {start_idx} ({checkpoint['indexed_count']} déjà indexés)\")\n\n# --- Récupère les product_ids déjà indexés (dedup) ---\nexisting_ids = set()\nfor obj in collection.iterator(return_properties=[\"product_id\"]):\n    pid = obj.properties.get(\"product_id\", \"\")\n    if pid:\n        existing_ids.add(pid)\nprint(f\"Déjà indexés: {len(existing_ids)} produits — on skip ceux-là\")\n\nindexed = checkpoint[\"indexed_count\"] if not RECREATE else 0\nerrors = checkpoint[\"errors\"] if not RECREATE else 0\nalready = 0\nstart_time = time.time()\n\n# --- Build encode tasks from selected indices ---\nrandom.seed(42)\nencode_tasks = []  # list of (ds_idx, product_id, meta)\n\nfor task_idx, ds_idx in enumerate(selected_indices):\n    if task_idx < start_idx:\n        continue\n\n    row = ds[ds_idx]\n    item_id = row[\"item_ID\"]\n    product_id = item_id.rsplit(\"_\", 2)[0]\n\n    if product_id in existing_ids:\n        already += 1\n        continue\n\n    category2 = row.get(\"category2\", \"\") or \"\"\n    color = row.get(\"color\", \"\") or \"\"\n    category1 = row.get(\"category1\", \"\") or \"\"\n    description = row.get(\"description\", \"\") or \"\"\n\n    # product_name = \"Color Category\" capitalisé\n    name_parts = [p for p in [color, category2] if p]\n    product_name = \" \".join(name_parts).title() if name_parts else item_id\n\n    # Prix et marque aléatoires\n    price_range = PRICE_RANGES.get(category2, (20, 100))\n    price = round(random.uniform(*price_range), 2)\n    brand = random.choice(BRANDS)\n\n    # Gender depuis category1\n    gender_raw = (category1 or \"\").strip().upper()\n    if gender_raw == \"WOMEN\":\n        gender = \"women\"\n    elif gender_raw == \"MEN\":\n        gender = \"men\"\n    else:\n        gender = None\n\n    meta = {\n        \"product_id\": product_id,\n        \"product_name\": product_name,\n        \"category\": category2,\n        \"color\": color,\n        \"price\": price,\n        \"brand\": brand,\n        \"product_url\": \"\",\n        \"description\": description,  # will be enriched by tagging\n        \"gender\": gender,\n        \"occasion\": \"\",  # will be set by tagging\n        \"size\": \"\",\n        \"image_index\": 0,\n        \"_task_idx\": task_idx,\n        \"_ds_idx\": ds_idx,\n    }\n    encode_tasks.append((ds_idx, product_id, meta))\n\nprint(f\"{already} produits déjà indexés (skip)\")\nprint(f\"{len(encode_tasks)} produits à encoder\")\nprint(f\"Poids image/texte: {IMAGE_WEIGHT:.1f} / {1 - IMAGE_WEIGHT:.1f}\")\nprint(f\"Pipeline: FC image encode → zero-shot tag → enriched desc → 2-model encode → Weaviate\")\nprint(f\"Config: ENCODE_BATCH={ENCODE_BATCH_SIZE}, WEAVIATE_BATCH={WEAVIATE_BATCH_SIZE}\")\n\n# --- Batch encode (GPU) + tagging + push Weaviate ---\nweaviate_queue = []\nlast_task_idx = start_idx\nfc_type, fc_mdl, fc_proc, _, _ = MODELS[VECTOR_FASHION_CLIP]\n\n\ndef flush_weaviate():\n    global weaviate_queue, indexed, last_task_idx\n    if not weaviate_queue:\n        return\n    with collection.batch.dynamic() as batch:\n        for doc in weaviate_queue:\n            vecs = doc.pop(\"_vectors\")\n            tidx = doc.pop(\"_task_idx\", 0)\n            doc.pop(\"_ds_idx\", None)\n            batch.add_object(properties=doc, vector=vecs)\n            last_task_idx = max(last_task_idx, tidx)\n    indexed += len(weaviate_queue)\n    save_checkpoint(last_task_idx, indexed, errors)\n    weaviate_queue = []\n\n\npbar = tqdm(total=len(encode_tasks), desc=\"Tag + Encode + Push (GPU)\")\n\nfor batch_start in range(0, len(encode_tasks), ENCODE_BATCH_SIZE):\n    batch_slice = encode_tasks[batch_start:batch_start + ENCODE_BATCH_SIZE]\n\n    batch_images = []\n    batch_metas = []\n\n    for ds_idx, product_id, meta in batch_slice:\n        try:\n            row = ds[ds_idx]\n            img = row[\"image\"]\n            if not isinstance(img, Image.Image):\n                img = Image.open(io.BytesIO(img)).convert(\"RGB\")\n            else:\n                img = img.convert(\"RGB\")\n\n            w, h = img.size\n            meta = meta.copy()\n            meta[\"thumbnail_base64\"] = make_thumbnail_b64(img)\n            meta[\"width\"] = w\n            meta[\"height\"] = h\n            meta[\"filename\"] = f\"{product_id}.jpg\"\n            meta[\"path\"] = \"\"\n            meta[\"indexed_at\"] = datetime.now(timezone.utc).isoformat()\n\n            batch_images.append(img)\n            batch_metas.append(meta)\n        except Exception as e:\n            errors += 1\n            if errors <= 10:\n                tqdm.write(f\"  Load error (idx={ds_idx}): {e}\")\n\n    if not batch_images:\n        pbar.update(len(batch_slice))\n        continue\n\n    try:\n        # Step 1: Encode images with Fashion CLIP (once, reused for tagging + vectorization)\n        with torch.no_grad():\n            fc_image_vecs = _encode_images_batch(fc_type, fc_mdl, fc_proc, batch_images)\n\n        # Step 2: Zero-shot tagging\n        batch_tags = tag_images_batch(fc_image_vecs, _label_embeddings)\n\n        # Step 3: Build enriched descriptions + occasion + gender from tags\n        batch_texts = []\n        for meta, tags in zip(batch_metas, batch_tags):\n            original_desc = meta.get(\"description\", \"\")\n            gender = meta.get(\"gender\")  # from category1\n            enriched_desc, occasion, tagged_gender = build_enriched_description(tags, original_desc)\n            meta[\"description\"] = enriched_desc\n            meta[\"occasion\"] = occasion\n            meta[\"gender\"] = gender or tagged_gender  # category1 en priorité, modèle en fallback\n\n            # Text for vector encoding = enriched description + product_name + category + color\n            vec_text_parts = [\n                meta.get(\"product_name\", \"\"),\n                meta.get(\"category\", \"\"),\n                meta.get(\"color\", \"\"),\n                enriched_desc,\n            ]\n            vec_text = \" \".join(p for p in vec_text_parts if p).strip()\n            batch_texts.append(vec_text)\n\n        # Step 4: Encode with 2 models (FC image vecs reused from step 1)\n        batch_vectors = encode_combined_batch(\n            batch_images, batch_texts,\n            fc_image_vecs_precomputed=fc_image_vecs,\n        )\n\n        for meta, vectors in zip(batch_metas, batch_vectors):\n            meta[\"_vectors\"] = {k: v.tolist() for k, v in vectors.items()}\n            weaviate_queue.append(meta)\n\n            if len(weaviate_queue) >= WEAVIATE_BATCH_SIZE:\n                flush_weaviate()\n\n        # Print first item of first batch as example\n        if batch_start == 0 and batch_metas:\n            tqdm.write(f\"\\n--- Example enriched item ---\")\n            tqdm.write(f\"  product_name: {batch_metas[0].get('product_name', '')}\")\n            tqdm.write(f\"  category: {batch_metas[0].get('category', '')}\")\n            tqdm.write(f\"  gender: {batch_metas[0].get('gender', '')}\")\n            tqdm.write(f\"  description: {batch_metas[0].get('description', '')[:200]}...\")\n            tqdm.write(f\"  occasion: {batch_metas[0].get('occasion', '')}\")\n            tqdm.write(f\"  tags: {batch_tags[0]}\")\n            tqdm.write(f\"---\\n\")\n\n    except Exception as e:\n        errors += len(batch_images)\n        if errors <= 10:\n            tqdm.write(f\"  Batch encode error: {e}\")\n\n    pbar.update(len(batch_slice))\n\n# Flush remaining\nflush_weaviate()\n\npbar.close()\nelapsed = time.time() - start_time\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Indexation terminée !\")\nprint(f\"Déjà présents: {already}\")\nprint(f\"Nouveaux produits indexés: {indexed}\")\nprint(f\"Erreurs: {errors}\")\nprint(f\"Temps: {elapsed:.0f}s ({indexed / max(elapsed, 1):.1f} produits/sec)\")\nprint(f\"{'='*50}\")\nclear_checkpoint()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 10. Vérification"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.collections.get(COLLECTION_NAME)\n",
    "stats = collection.aggregate.over_all(total_count=True)\n",
    "print(f\"Collection '{COLLECTION_NAME}': {stats.total_count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from weaviate.classes.query import MetadataQuery\nfrom IPython.display import display, HTML\n\nquery = \"black leather jacket\"\n\ncollection = client.collections.get(COLLECTION_NAME)\n\n# --- 1. Encode query with all 2 models ---\nquery_vectors = {}\nfor vec_name, (model_type, model, processor, tokenizer, max_len) in MODELS.items():\n    with torch.no_grad():\n        if model_type == \"openclip\":\n            tokens = tokenizer([query]).to(device)\n            features = model.encode_text(tokens)\n        else:\n            tok = getattr(processor, \"tokenizer\", processor)\n            inputs = tok(\n                [query], return_tensors=\"pt\", padding=True,\n                truncation=True, max_length=max_len,\n            )\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            features = model.get_text_features(**inputs)\n        features = _to_tensor(features)\n        features = features / features.norm(p=2, dim=-1, keepdim=True)\n        query_vectors[vec_name] = features.cpu().float().numpy().flatten()\n\n# --- 2. BM25 search (with boosts) ---\nbm25_results = collection.query.bm25(\n    query=query,\n    query_properties=[\"product_name^3\", \"category^2\", \"color^2\", \"description\", \"occasion\"],\n    limit=60,\n    return_metadata=MetadataQuery(score=True),\n)\nprint(f\"BM25: {len(bm25_results.objects)} results\")\n\n# Show BM25 top hits to debug\nfor i, obj in enumerate(bm25_results.objects[:5]):\n    p = obj.properties\n    print(f\"  BM25 #{i+1}: score={obj.metadata.score:.2f} | \"\n          f\"name={p.get('product_name') or ''} | \"\n          f\"color={p.get('color') or ''} | \"\n          f\"cat={p.get('category') or ''} | \"\n          f\"desc={str(p.get('description') or '')[:80]}\")\n\n# --- 3. near_vector per model ---\nvector_results = {}\nfor vec_name, vec in query_vectors.items():\n    results = collection.query.near_vector(\n        near_vector=vec.tolist(),\n        target_vector=vec_name,\n        limit=60,\n        return_metadata=MetadataQuery(distance=True),\n    )\n    vector_results[vec_name] = results\n    print(f\"{vec_name}: {len(results.objects)} results\")\n\n# --- 4. RRF Fusion (BM25 counted 2x for more keyword weight) ---\ndef rrf_fusion(result_lists, limit=20, rrf_k=60):\n    scores = {}\n    result_map = {}\n    for results in result_lists:\n        for rank, obj in enumerate(results):\n            key = (obj.properties.get(\"product_id\") or obj.properties.get(\"filename\") or str(rank))\n            scores[key] = scores.get(key, 0) + 1 / (rrf_k + rank + 1)\n            if key not in result_map:\n                result_map[key] = obj\n    sorted_keys = sorted(scores, key=lambda x: scores[x], reverse=True)\n    return [(result_map[key], scores[key]) for key in sorted_keys[:limit]]\n\n# BM25 counted twice (2/4 weight) vs 2 vectors (2/4 weight)\nall_lists = [bm25_results.objects, bm25_results.objects]\nfor vec_name in query_vectors:\n    all_lists.append(vector_results[vec_name].objects)\n\nfused = rrf_fusion(all_lists, limit=20)\nprint(f\"\\nRRF Fusion: {len(all_lists)} sources (BM25 x2 + 2 vectors) -> {len(fused)} results\")\n\n# --- Helper to safely get string properties ---\ndef _p(props, key, max_len=0):\n    val = props.get(key) or \"\"\n    return val[:max_len] if max_len else val\n\n# --- 5. Display hybrid results ---\nhtml = f'<h3>Hybrid Search: \"{query}\" (BM25 x2 + 2 vectors + RRF)</h3>'\nhtml += '<div style=\"display:flex; gap:10px; flex-wrap:wrap;\">'\nfor obj, rrf_score in fused[:20]:\n    p = obj.properties\n    thumb = _p(p, \"thumbnail_base64\")\n    name = _p(p, \"product_name\", 45)\n    color = _p(p, \"color\")\n    category = _p(p, \"category\")\n    occasion = _p(p, \"occasion\", 50)\n    desc = _p(p, \"description\", 80)\n    price = p.get(\"price\")\n    price_str = f\"£{price:.2f}\" if price else \"\"\n    html += f'''\n    <div style=\"text-align:center; width:170px; border:1px solid #eee; padding:5px; border-radius:8px;\">\n        <img src=\"data:image/jpeg;base64,{thumb}\" style=\"max-width:150px; max-height:150px;\"/>\n        <div style=\"font-size:11px; font-weight:bold;\">{name}</div>\n        <div style=\"font-size:10px; color:#666;\">{color} | {category}</div>\n        <div style=\"font-size:10px; color:#999;\">{occasion}</div>\n        <div style=\"font-size:9px; color:#aaa;\">{desc}</div>\n        <div style=\"font-size:11px; color:#333;\">{price_str} — RRF: {rrf_score:.4f}</div>\n    </div>'''\nhtml += '</div>'\ndisplay(HTML(html))\n\n# --- 6. Per-source comparison ---\nsources = [(\"BM25\", bm25_results.objects[:5])]\nfor vn in query_vectors:\n    sources.append((vn, vector_results[vn].objects[:5]))\n\nfor source_name, source_results in sources:\n    html = f'<h4>{source_name} top 5</h4><div style=\"display:flex; gap:8px; flex-wrap:wrap;\">'\n    for obj in source_results:\n        p = obj.properties\n        thumb = _p(p, \"thumbnail_base64\")\n        name = _p(p, \"product_name\", 35)\n        color = _p(p, \"color\")\n        html += f'''\n        <div style=\"text-align:center; width:130px;\">\n            <img src=\"data:image/jpeg;base64,{thumb}\" style=\"max-width:120px; max-height:120px;\"/>\n            <div style=\"font-size:10px;\">{name}</div>\n            <div style=\"font-size:10px; color:#888;\">{color}</div>\n        </div>'''\n    html += '</div>'\n    display(HTML(html))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "_keepalive_stop.set()  # Stop le thread keep-alive\nclient.close()\nprint(\"Connexion Weaviate fermee.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}