{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion Search — Indexation ASOS depuis Colab (GPU)\n",
    "\n",
    "Ce notebook indexe le dataset [ASOS e-commerce](https://huggingface.co/datasets/UniqueData/asos-e-commerce-dataset) dans Weaviate en utilisant [Fashion CLIP](https://huggingface.co/patrickjohncyh/fashion-clip) sur GPU Colab.\n",
    "\n",
    "**Architecture :**\n",
    "- **Colab (GPU)** : encode les images avec Fashion CLIP → pousse les vecteurs dans Weaviate\n",
    "- **GCP (CPU)** : Weaviate + app FastAPI pour servir les recherches\n",
    "\n",
    "**Prérequis :**\n",
    "- Runtime GPU activé (Runtime → Change runtime type → T4 GPU)\n",
    "- Weaviate accessible sur ta VM GCP (port 8080 ouvert dans le firewall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q weaviate-client>=4.0 transformers torch torchvision Pillow datasets requests tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Renseigne l'IP externe de ta VM GCP où tourne Weaviate.\n",
    "\n",
    "**Important** : le port `8080` (HTTP) et `50051` (gRPC) doivent être ouverts dans le firewall GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "GCP_EXTERNAL_IP = \"\"   # ex: \"34.56.78.90\"\n",
    "\n",
    "WEAVIATE_HTTP_PORT = 8080\n",
    "WEAVIATE_GRPC_PORT = 50051\n",
    "\n",
    "COLLECTION_NAME = \"FashionCollection\"\n",
    "MODEL_NAME = \"patrickjohncyh/fashion-clip\"\n",
    "\n",
    "MAX_ITEMS = 2000              # produits à indexer (None = tout le dataset)\n",
    "MAX_IMAGES_PER_PRODUCT = 1    # images par produit\n",
    "BATCH_SIZE = 32               # taille des batches GPU\n",
    "WEAVIATE_BATCH_SIZE = 100     # taille des batches Weaviate\n",
    "THUMBNAIL_SIZE = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vérification GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\n\nif torch.cuda.is_available():\n    device = \"cuda\"\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nelse:\n    device = \"cpu\"\n    print(\"Pas de GPU, l'indexation sera lente.\")\n\nprint(f\"Device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chargement du modèle Fashion CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "print(f\"Chargement de {MODEL_NAME}...\")\n",
    "model = CLIPModel.from_pretrained(MODEL_NAME)\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    model = model.half()  # FP16\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "VECTOR_DIM = model.config.projection_dim\n",
    "print(f\"Modele charge - dimension vecteur: {VECTOR_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Connexion a Weaviate sur GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.config import Configure, DataType, Property, VectorDistances\n",
    "\n",
    "assert GCP_EXTERNAL_IP, \"Renseigne GCP_EXTERNAL_IP dans la cellule de configuration.\"\n",
    "\n",
    "client = weaviate.connect_to_custom(\n",
    "    http_host=GCP_EXTERNAL_IP,\n",
    "    http_port=WEAVIATE_HTTP_PORT,\n",
    "    http_secure=False,\n",
    "    grpc_host=GCP_EXTERNAL_IP,\n",
    "    grpc_port=WEAVIATE_GRPC_PORT,\n",
    "    grpc_secure=False,\n",
    ")\n",
    "\n",
    "print(f\"Connecte a Weaviate sur {GCP_EXTERNAL_IP}\" if client.is_ready() else \"Echec de connexion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creation du schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprime et recree la collection\n",
    "if client.collections.exists(COLLECTION_NAME):\n",
    "    client.collections.delete(COLLECTION_NAME)\n",
    "    print(f\"Collection '{COLLECTION_NAME}' supprimee.\")\n",
    "\n",
    "client.collections.create(\n",
    "    name=COLLECTION_NAME,\n",
    "    properties=[\n",
    "        Property(name=\"filename\", data_type=DataType.TEXT),\n",
    "        Property(name=\"path\", data_type=DataType.TEXT),\n",
    "        Property(name=\"thumbnail_base64\", data_type=DataType.TEXT),\n",
    "        Property(name=\"width\", data_type=DataType.INT),\n",
    "        Property(name=\"height\", data_type=DataType.INT),\n",
    "        Property(name=\"indexed_at\", data_type=DataType.TEXT),\n",
    "        Property(name=\"product_id\", data_type=DataType.TEXT),\n",
    "        Property(name=\"product_name\", data_type=DataType.TEXT),\n",
    "        Property(name=\"category\", data_type=DataType.TEXT),\n",
    "        Property(name=\"color\", data_type=DataType.TEXT),\n",
    "        Property(name=\"size\", data_type=DataType.TEXT),\n",
    "        Property(name=\"price\", data_type=DataType.NUMBER),\n",
    "        Property(name=\"brand\", data_type=DataType.TEXT),\n",
    "        Property(name=\"product_url\", data_type=DataType.TEXT),\n",
    "        Property(name=\"description\", data_type=DataType.TEXT),\n",
    "        Property(name=\"image_index\", data_type=DataType.INT),\n",
    "        Property(name=\"gender\", data_type=DataType.TEXT),\n",
    "    ],\n",
    "    vectorizer_config=Configure.Vectorizer.none(),\n",
    "    vector_index_config=Configure.VectorIndex.hnsw(\n",
    "        distance_metric=VectorDistances.COSINE\n",
    "    ),\n",
    ")\n",
    "print(f\"Collection '{COLLECTION_NAME}' creee (dim={VECTOR_DIM}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Chargement du dataset ASOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Chargement du dataset ASOS...\")\n",
    "dataset = load_dataset(\"UniqueData/asos-e-commerce-dataset\", split=\"train\")\n",
    "print(f\"Dataset charge: {len(dataset)} produits\")\n",
    "\n",
    "if MAX_ITEMS:\n",
    "    dataset = dataset.select(range(min(MAX_ITEMS, len(dataset))))\n",
    "    print(f\"Limite a {len(dataset)} produits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import base64\nimport io\nfrom datetime import datetime\n\nimport requests\nfrom PIL import Image\n\n\ndef extract_images(images_field):\n    \"\"\"Extract image URLs (dataset returns a Python list directly).\"\"\"\n    if not images_field:\n        return []\n    if isinstance(images_field, list):\n        return [u for u in images_field if isinstance(u, str) and u.startswith(\"http\")]\n    if isinstance(images_field, str) and images_field.startswith(\"http\"):\n        return [images_field]\n    return []\n\n\ndef extract_description(desc_field):\n    \"\"\"Extract brand + text from description (list of dicts in the dataset).\"\"\"\n    if not desc_field:\n        return None, \"\"\n    if isinstance(desc_field, list):\n        brand = None\n        texts = []\n        for entry in desc_field:\n            if isinstance(entry, dict):\n                for key, val in entry.items():\n                    if \"brand\" in key.lower():\n                        brand = str(val) if val else None\n                    else:\n                        texts.append(str(val))\n        return brand, \" \".join(texts)\n    return None, str(desc_field)\n\n\ndef extract_price(price_field):\n    \"\"\"Extract price (already a float in the dataset).\"\"\"\n    if price_field is None:\n        return None\n    try:\n        val = float(price_field)\n        return val if val > 0 else None\n    except (ValueError, TypeError):\n        return None\n\n\ndef detect_gender(product_name, category):\n    text = f\"{product_name} {category}\".lower()\n    for kw in [\"women's\", \"womens\", \"female\", \"femme\", \" woman \", \"for women\", \"ladies\", \"maternity\"]:\n        if kw in text:\n            return \"women\"\n    for kw in [\"men's\", \"mens\", \"male\", \"homme\", \" man \", \"for men\"]:\n        if kw in text:\n            return \"men\"\n    return None\n\n\ndef download_image(url, timeout=10):\n    try:\n        r = requests.get(url, timeout=timeout, stream=True)\n        r.raise_for_status()\n        return Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n    except Exception:\n        return None\n\n\ndef make_thumbnail_b64(image, size=THUMBNAIL_SIZE):\n    img = image.copy()\n    img.thumbnail((size, size))\n    if img.mode in (\"RGBA\", \"P\"):\n        img = img.convert(\"RGB\")\n    buf = io.BytesIO()\n    img.save(buf, format=\"JPEG\", quality=85)\n    return base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n\n\ndef encode_images_batch(images):\n    with torch.no_grad():\n        inputs = processor(images=images, return_tensors=\"pt\")\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        features = model.get_image_features(**inputs)\n        features = features / features.norm(p=2, dim=-1, keepdim=True)\n    return features.cpu().float().numpy()\n\n\nprint(\"OK\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Indexation (GPU batch encoding → Weaviate sur GCP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nfrom tqdm.auto import tqdm\n\ncollection = client.collections.get(COLLECTION_NAME)\n\nimage_batch = []\nmeta_batch = []\nweaviate_queue = []\n\nindexed = 0\nskipped = 0\nstart_time = time.time()\n\n\ndef flush_gpu_batch():\n    global image_batch, meta_batch\n    if not image_batch:\n        return\n    vectors = encode_images_batch(image_batch)\n    for vec, meta in zip(vectors, meta_batch):\n        meta[\"vector\"] = vec.tolist()\n        weaviate_queue.append(meta)\n    image_batch = []\n    meta_batch = []\n\n\ndef flush_weaviate():\n    global weaviate_queue, indexed\n    if not weaviate_queue:\n        return\n    with collection.batch.dynamic() as batch:\n        for doc in weaviate_queue:\n            vec = doc.pop(\"vector\")\n            batch.add_object(properties=doc, vector=vec)\n    indexed += len(weaviate_queue)\n    weaviate_queue = []\n\n\npbar = tqdm(total=len(dataset), desc=\"Indexation\")\n\nfor item in dataset:\n    product_id = str(item.get(\"sku\", \"\"))\n    product_name = item.get(\"name\", \"\")\n    category = item.get(\"category\", \"\")\n    color = item.get(\"color\", \"\")\n    price = extract_price(item.get(\"price\"))\n    product_url = item.get(\"url\", \"\")\n\n    brand, description = extract_description(item.get(\"description\"))\n    image_urls = extract_images(item.get(\"images\"))\n    gender = detect_gender(product_name or \"\", category or \"\")\n\n    product_has_image = False\n\n    for idx, url in enumerate(image_urls[:MAX_IMAGES_PER_PRODUCT]):\n        img = download_image(url)\n        if img is None:\n            continue\n\n        product_has_image = True\n        w, h = img.size\n\n        meta = {\n            \"filename\": f\"{product_id}_{idx}.jpg\",\n            \"path\": url,\n            \"thumbnail_base64\": make_thumbnail_b64(img),\n            \"width\": w,\n            \"height\": h,\n            \"indexed_at\": datetime.utcnow().isoformat(),\n            \"product_id\": product_id,\n            \"product_name\": product_name or \"\",\n            \"category\": category or \"\",\n            \"color\": color or \"\",\n            \"price\": price,\n            \"brand\": brand or \"\",\n            \"product_url\": product_url or \"\",\n            \"description\": description,\n            \"image_index\": idx,\n            \"gender\": gender,\n        }\n\n        image_batch.append(img)\n        meta_batch.append(meta)\n\n        if len(image_batch) >= BATCH_SIZE:\n            flush_gpu_batch()\n\n        if len(weaviate_queue) >= WEAVIATE_BATCH_SIZE:\n            flush_weaviate()\n\n    if not product_has_image:\n        skipped += 1\n\n    pbar.update(1)\n\nflush_gpu_batch()\nflush_weaviate()\n\npbar.close()\nelapsed = time.time() - start_time\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Indexation terminee !\")\nprint(f\"Images indexees: {indexed}\")\nprint(f\"Produits sans image: {skipped}\")\nprint(f\"Temps: {elapsed:.0f}s ({indexed / max(elapsed, 1):.1f} images/sec)\")\nprint(f\"{'='*50}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.collections.get(COLLECTION_NAME)\n",
    "stats = collection.aggregate.over_all(total_count=True)\n",
    "print(f\"Collection '{COLLECTION_NAME}': {stats.total_count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.query import MetadataQuery\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "query = \"black leather jacket\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = processor(text=[query], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    features = model.get_text_features(**inputs)\n",
    "    features = features / features.norm(p=2, dim=-1, keepdim=True)\n",
    "    query_vector = features.cpu().float().numpy().flatten().tolist()\n",
    "\n",
    "results = collection.query.near_vector(\n",
    "    near_vector=query_vector,\n",
    "    limit=5,\n",
    "    return_metadata=MetadataQuery(distance=True),\n",
    ")\n",
    "\n",
    "print(f\"Resultats pour: '{query}'\\n\")\n",
    "\n",
    "html = '<div style=\"display:flex; gap:10px; flex-wrap:wrap;\">'\n",
    "for obj in results.objects:\n",
    "    p = obj.properties\n",
    "    score = f\"{1 - (obj.metadata.distance or 0):.3f}\"\n",
    "    thumb = p.get(\"thumbnail_base64\", \"\")\n",
    "    name = p.get(\"product_name\", \"\")\n",
    "    price = p.get(\"price\")\n",
    "    price_str = f\"\\u00a3{price:.2f}\" if price else \"\"\n",
    "    html += f'''\n",
    "    <div style=\"text-align:center; width:160px;\">\n",
    "        <img src=\"data:image/jpeg;base64,{thumb}\" style=\"max-width:150px; max-height:150px;\"/>\n",
    "        <div style=\"font-size:11px;\">{name[:40]}</div>\n",
    "        <div style=\"font-size:11px; color:gray;\">{price_str} - score: {score}</div>\n",
    "    </div>'''\n",
    "html += '</div>'\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "print(\"Connexion Weaviate fermee.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}