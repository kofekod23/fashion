{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fashion Search — Indexation ASOS depuis Colab (GPU)\n\nCe notebook indexe le dataset [ASOS e-commerce](https://huggingface.co/datasets/UniqueData/asos-e-commerce-dataset) dans Weaviate avec **3 named vectors** :\n- **fashion_clip** (512d) — `patrickjohncyh/fashion-clip`\n- **marqo_clip** (512d) — `Marqo/marqo-fashionCLIP`\n- **siglip2** (1152d) — `google/siglip2-so400m-patch14-384`\n\n**Features :**\n- **Batch GPU encoding** : N images encodées d'un coup par modèle (vs 1 par 1)\n- 3 modèles en parallèle via CUDA streams + ThreadPoolExecutor\n- Combined image + text embeddings (0.5 img + 0.5 txt: category, color, occasion)\n- Champ `occasion` auto-généré (quand porter ce vêtement : sport, office, party...)\n- Descriptions texte pour BM25 search (color, category, description, occasion)\n- Checkpoint / reprise automatique\n- Keep-alive Weaviate (ping toutes les 2 min)\n\n**Architecture :**\n- **Colab (GPU)** : encode les images en batch avec les 3 modèles → pousse les vecteurs dans Weaviate\n- **GCP (CPU)** : Weaviate + app FastAPI pour servir les recherches\n\n**Prérequis :**\n- Runtime GPU activé (Runtime → Change runtime type → T4 GPU)\n- Weaviate accessible sur ta VM GCP (port 8080 ouvert dans le firewall)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install deps (sans toucher au torch pre-installe sur Colab)\n!pip install -q weaviate-client>=4.0 transformers Pillow datasets requests tqdm timm ftfy regex\n!pip install -q --no-deps open-clip-torch"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Renseigne l'IP externe de ta VM GCP où tourne Weaviate.\n",
    "\n",
    "**Important** : le port `8080` (HTTP) et `50051` (gRPC) doivent être ouverts dans le firewall GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- CONFIGURATION ---\nGCP_EXTERNAL_IP = \"\"   # ex: \"34.56.78.90\"\n\nWEAVIATE_HTTP_PORT = 8080\nWEAVIATE_GRPC_PORT = 50051\n\nCOLLECTION_NAME = \"FashionCollection\"\n\n# Modèles\nMODEL_FASHION_CLIP = \"patrickjohncyh/fashion-clip\"\nMODEL_MARQO_CLIP = \"Marqo/marqo-fashionCLIP\"\nMODEL_SIGLIP2 = \"google/siglip2-so400m-patch14-384\"\n\n# Named vector keys\nVECTOR_FASHION_CLIP = \"fashion_clip\"\nVECTOR_MARQO_CLIP = \"marqo_clip\"\nVECTOR_SIGLIP2 = \"siglip2\"\nALL_VECTOR_NAMES = [VECTOR_FASHION_CLIP, VECTOR_MARQO_CLIP, VECTOR_SIGLIP2]\n\n# --- Tuning A100 40GB + 83GB RAM ---\nMAX_ITEMS = None              # tout le dataset (~45k produits)\nMAX_IMAGES_PER_PRODUCT = 1    # images par produit\nWEAVIATE_BATCH_SIZE = 500     # objets par flush vers Weaviate\nDOWNLOAD_WORKERS = 128        # téléchargements parallèles (phase 1)\nENCODE_BATCH_SIZE = 128       # images par batch GPU (128 pour A100, 32 pour T4)\nTHUMBNAIL_SIZE = 150\nIMAGE_WEIGHT = 0.5            # 50/50 image et texte (category + color + occasion)\nRECREATE = False              # False = reprise, True = tout recréer\n\n# Répertoire local pour les images pré-téléchargées\nIMAGE_CACHE_DIR = \"/content/asos_images\"\n\n# Checkpoint (reprise automatique)\nCHECKPOINT_FILE = \"/content/index_checkpoint.json\"\nKEEPALIVE_INTERVAL = 120      # secondes"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vérification GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\n\nif torch.cuda.is_available():\n    device = \"cuda\"\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nelse:\n    device = \"cpu\"\n    print(\"Pas de GPU, l'indexation sera lente.\")\n\nprint(f\"Device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Chargement des 3 modèles"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import CLIPModel, CLIPProcessor, AutoModel, AutoProcessor\nimport open_clip\n\n# Fashion CLIP (512d) — HuggingFace format\nprint(f\"Chargement de {MODEL_FASHION_CLIP}...\")\nfc_model = CLIPModel.from_pretrained(MODEL_FASHION_CLIP).to(device).eval()\nfc_processor = CLIPProcessor.from_pretrained(MODEL_FASHION_CLIP)\n\n# Marqo FashionCLIP (512d) — OpenCLIP format\nprint(f\"Chargement de {MODEL_MARQO_CLIP}...\")\nmq_model, _, mq_preprocess = open_clip.create_model_and_transforms(\n    f\"hf-hub:{MODEL_MARQO_CLIP}\", device=device\n)\nmq_tokenizer = open_clip.get_tokenizer(f\"hf-hub:{MODEL_MARQO_CLIP}\")\nmq_model = mq_model.eval()\n\n# SigLIP2-SO400M (1152d) — HuggingFace format\nprint(f\"Chargement de {MODEL_SIGLIP2}...\")\nsl_model = AutoModel.from_pretrained(MODEL_SIGLIP2).to(device).eval()\nsl_processor = AutoProcessor.from_pretrained(MODEL_SIGLIP2)\n\n# Dict unifie : (type, model, processor/preprocess, tokenizer, max_text_tokens)\nMODELS = {\n    VECTOR_FASHION_CLIP: (\"hf\", fc_model, fc_processor, None, 77),\n    VECTOR_MARQO_CLIP: (\"openclip\", mq_model, mq_preprocess, mq_tokenizer, 77),\n    VECTOR_SIGLIP2: (\"hf\", sl_model, sl_processor, None, 64),\n}\n\n# A100 40GB : ~5GB en FP32 pour les 3 modeles, pas besoin de FP16\nif device == \"cuda\":\n    vram_used = torch.cuda.memory_allocated() / 1024**3\n    vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"\\n3 modeles charges sur {device} — VRAM: {vram_used:.1f}/{vram_total:.0f} GB\")\nelse:\n    print(f\"\\n3 modeles charges sur {device}.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Connexion a Weaviate sur GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import weaviate\nimport json\nimport os\nimport threading\nfrom datetime import datetime, timezone\nfrom weaviate.classes.config import Configure, DataType, Property, VectorDistances\n\nassert GCP_EXTERNAL_IP, \"Renseigne GCP_EXTERNAL_IP dans la cellule de configuration.\"\n\nclient = weaviate.connect_to_custom(\n    http_host=GCP_EXTERNAL_IP,\n    http_port=WEAVIATE_HTTP_PORT,\n    http_secure=False,\n    grpc_host=GCP_EXTERNAL_IP,\n    grpc_port=WEAVIATE_GRPC_PORT,\n    grpc_secure=False,\n)\n\nprint(f\"Connecte a Weaviate sur {GCP_EXTERNAL_IP}\" if client.is_ready() else \"Echec de connexion\")\n\n# --- Keep-alive (ping toutes les 2 min, auto-reconnect) ---\n_keepalive_stop = threading.Event()\n\ndef start_keepalive(wv_client, interval=KEEPALIVE_INTERVAL):\n    def _run():\n        while not _keepalive_stop.wait(interval):\n            try:\n                if not wv_client.is_ready():\n                    raise ConnectionError(\"not ready\")\n            except Exception as e:\n                print(f\"\\n[keep-alive] Reconnexion... ({e})\")\n                try:\n                    wv_client.close()\n                except Exception:\n                    pass\n                wv_client.connect_to_custom(\n                    http_host=GCP_EXTERNAL_IP,\n                    http_port=WEAVIATE_HTTP_PORT,\n                    http_secure=False,\n                    grpc_host=GCP_EXTERNAL_IP,\n                    grpc_port=WEAVIATE_GRPC_PORT,\n                    grpc_secure=False,\n                )\n    t = threading.Thread(target=_run, daemon=True)\n    t.start()\n    return t\n\nstart_keepalive(client)\nprint(\"Keep-alive demarre (ping toutes les 2 min)\")\n\n# --- Checkpoint helpers ---\ndef load_checkpoint():\n    try:\n        if os.path.exists(CHECKPOINT_FILE):\n            with open(CHECKPOINT_FILE) as f:\n                return json.load(f)\n    except Exception:\n        pass\n    return {\"last_index\": -1, \"indexed_count\": 0, \"errors\": 0}\n\ndef save_checkpoint(last_index, indexed_count, errors):\n    with open(CHECKPOINT_FILE, \"w\") as f:\n        json.dump({\"last_index\": last_index, \"indexed_count\": indexed_count,\n                    \"errors\": errors, \"timestamp\": datetime.now(timezone.utc).isoformat()}, f)\n\ndef clear_checkpoint():\n    if os.path.exists(CHECKPOINT_FILE):\n        os.remove(CHECKPOINT_FILE)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creation du schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_schema(wv_client, collection_name, vector_names=None):\n    \"\"\"Crée la collection avec named vectors.\"\"\"\n    if vector_names is None:\n        vector_names = ALL_VECTOR_NAMES\n\n    named_vectors = [\n        Configure.NamedVectors.none(\n            name=name,\n            vector_index_config=Configure.VectorIndex.hnsw(\n                distance_metric=VectorDistances.COSINE\n            ),\n        )\n        for name in vector_names\n    ]\n\n    wv_client.collections.create(\n        name=collection_name,\n        properties=[\n            Property(name=\"filename\", data_type=DataType.TEXT),\n            Property(name=\"path\", data_type=DataType.TEXT),\n            Property(name=\"thumbnail_base64\", data_type=DataType.TEXT),\n            Property(name=\"width\", data_type=DataType.INT),\n            Property(name=\"height\", data_type=DataType.INT),\n            Property(name=\"indexed_at\", data_type=DataType.TEXT),\n            Property(name=\"product_id\", data_type=DataType.TEXT),\n            Property(name=\"product_name\", data_type=DataType.TEXT),\n            Property(name=\"category\", data_type=DataType.TEXT),\n            Property(name=\"color\", data_type=DataType.TEXT),\n            Property(name=\"size\", data_type=DataType.TEXT),\n            Property(name=\"price\", data_type=DataType.NUMBER),\n            Property(name=\"brand\", data_type=DataType.TEXT),\n            Property(name=\"product_url\", data_type=DataType.TEXT),\n            Property(name=\"description\", data_type=DataType.TEXT),\n            Property(name=\"image_index\", data_type=DataType.INT),\n            Property(name=\"gender\", data_type=DataType.TEXT),\n            Property(name=\"occasion\", data_type=DataType.TEXT),\n        ],\n        vectorizer_config=named_vectors,\n    )\n    print(f\"Collection '{collection_name}' creee avec vecteurs: {vector_names}\")\n\n\nif RECREATE:\n    if client.collections.exists(COLLECTION_NAME):\n        client.collections.delete(COLLECTION_NAME)\n        print(f\"Collection '{COLLECTION_NAME}' supprimee.\")\n    create_schema(client, COLLECTION_NAME)\n    clear_checkpoint()\nelif not client.collections.exists(COLLECTION_NAME):\n    create_schema(client, COLLECTION_NAME)\n    clear_checkpoint()\nelse:\n    print(f\"Collection '{COLLECTION_NAME}' existe deja, on garde les donnees.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Chargement du dataset ASOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Chargement du dataset ASOS...\")\n",
    "dataset = load_dataset(\"UniqueData/asos-e-commerce-dataset\", split=\"train\")\n",
    "print(f\"Dataset charge: {len(dataset)} produits\")\n",
    "\n",
    "if MAX_ITEMS:\n",
    "    dataset = dataset.select(range(min(MAX_ITEMS, len(dataset))))\n",
    "    print(f\"Limite a {len(dataset)} produits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 7b. Phase 1 : Pré-téléchargement de toutes les images sur disque\n\nTélécharge toutes les images en parallèle AVANT l'encodage GPU.\nComme ça le GPU ne dort jamais en attendant le réseau.\n~45k images × ~100-200KB = ~5-10GB sur disque, ça tient largement.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import ast\nimport io\nimport os\nimport time\nimport requests\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\nos.makedirs(IMAGE_CACHE_DIR, exist_ok=True)\n\n\ndef _download_and_save(args):\n    \"\"\"Download one image and save to disk. Returns (product_id, idx, success).\"\"\"\n    product_id, idx, url, filepath = args\n    if os.path.exists(filepath):\n        return product_id, idx, True  # deja telecharge\n    try:\n        r = requests.get(url, timeout=15, stream=True)\n        r.raise_for_status()\n        with open(filepath, \"wb\") as f:\n            for chunk in r.iter_content(8192):\n                f.write(chunk)\n        return product_id, idx, True\n    except Exception:\n        return product_id, idx, False\n\n\n# --- Collect all image URLs ---\ndownload_jobs = []\nno_image_count = 0\n\nfor product_idx in range(len(dataset)):\n    item = dataset[product_idx]\n    product_id = str(int(item.get(\"sku\", 0))) if item.get(\"sku\") else str(product_idx)\n\n    raw_images = item.get(\"images\", \"\")\n    if not raw_images:\n        no_image_count += 1\n        continue\n    if isinstance(raw_images, str):\n        try:\n            parsed = ast.literal_eval(raw_images)\n            image_urls = [u for u in parsed if isinstance(u, str) and u.startswith(\"http\")]\n        except (ValueError, SyntaxError):\n            image_urls = [raw_images] if raw_images.startswith(\"http\") else []\n    elif isinstance(raw_images, list):\n        image_urls = [u for u in raw_images if isinstance(u, str) and u.startswith(\"http\")]\n    else:\n        image_urls = []\n\n    if not image_urls:\n        no_image_count += 1\n        continue\n\n    for idx, url in enumerate(image_urls[:MAX_IMAGES_PER_PRODUCT]):\n        filepath = os.path.join(IMAGE_CACHE_DIR, f\"{product_id}_{idx}.jpg\")\n        download_jobs.append((product_id, idx, url, filepath))\n\n# Count already cached\nalready_cached = sum(1 for _, _, _, fp in download_jobs if os.path.exists(fp))\nto_download = len(download_jobs) - already_cached\n\nprint(f\"Total images: {len(download_jobs)}\")\nprint(f\"Deja sur disque: {already_cached}\")\nprint(f\"A telecharger: {to_download}\")\nprint(f\"Sans image: {no_image_count}\")\n\nif to_download > 0:\n    t0 = time.time()\n    success = 0\n    failed = 0\n\n    with ThreadPoolExecutor(max_workers=DOWNLOAD_WORKERS) as executor:\n        futures = {executor.submit(_download_and_save, job): job for job in download_jobs}\n        with tqdm(total=len(download_jobs), desc=\"Phase 1 : Telechargement\") as pbar:\n            for future in as_completed(futures):\n                _, _, ok = future.result()\n                if ok:\n                    success += 1\n                else:\n                    failed += 1\n                pbar.update(1)\n\n    elapsed = time.time() - t0\n    print(f\"\\nTelechargement termine: {success} OK, {failed} echecs en {elapsed:.0f}s\")\nelse:\n    print(\"Toutes les images sont deja sur disque !\")\n\n# Disk usage\ntotal_size = sum(\n    os.path.getsize(os.path.join(IMAGE_CACHE_DIR, f))\n    for f in os.listdir(IMAGE_CACHE_DIR)\n    if f.endswith(\".jpg\")\n)\ncached_count = len([f for f in os.listdir(IMAGE_CACHE_DIR) if f.endswith(\".jpg\")])\nprint(f\"Cache: {cached_count} images, {total_size / 1024**3:.1f} GB\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import ast\nimport base64\nimport io\nimport re\nfrom contextlib import nullcontext\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nimport numpy as np\nimport requests\nfrom PIL import Image\n\n\ndef extract_images(images_field):\n    \"\"\"Extract image URLs (field is a string repr of a list).\"\"\"\n    if not images_field:\n        return []\n    if isinstance(images_field, str):\n        try:\n            parsed = ast.literal_eval(images_field)\n            if isinstance(parsed, list):\n                return [u for u in parsed if isinstance(u, str) and u.startswith(\"http\")]\n        except (ValueError, SyntaxError):\n            pass\n        if images_field.startswith(\"http\"):\n            return [images_field]\n    if isinstance(images_field, list):\n        return [u for u in images_field if isinstance(u, str) and u.startswith(\"http\")]\n    return []\n\n\ndef extract_description(desc_field):\n    \"\"\"Extract brand + text from description (string repr of list of dicts).\"\"\"\n    if not desc_field:\n        return None, \"\"\n    data = desc_field\n    if isinstance(data, str):\n        try:\n            data = ast.literal_eval(data)\n        except (ValueError, SyntaxError):\n            return None, str(desc_field)\n    if isinstance(data, list):\n        brand = None\n        texts = []\n        for entry in data:\n            if isinstance(entry, dict):\n                for key, val in entry.items():\n                    if \"brand\" in key.lower():\n                        brand = str(val) if val else None\n                    else:\n                        texts.append(str(val))\n        return brand, \" \".join(texts)\n    return None, str(desc_field)\n\n\ndef extract_price(price_field):\n    \"\"\"Extract price (string in the dataset, e.g. '49.99').\"\"\"\n    if price_field is None:\n        return None\n    try:\n        val = float(price_field)\n        return val if val > 0 else None\n    except (ValueError, TypeError):\n        return None\n\n\ndef detect_gender(product_name, category):\n    text = f\"{product_name} {category}\".lower()\n    for kw in [\"women's\", \"womens\", \"female\", \"femme\", \" woman \", \"for women\", \"ladies\", \"maternity\"]:\n        if kw in text:\n            return \"women\"\n    for kw in [\"men's\", \"mens\", \"male\", \"homme\", \" man \", \"for men\"]:\n        if kw in text:\n            return \"men\"\n    return None\n\n\ndef detect_occasion(product_name, category):\n    \"\"\"Detect when/where to wear based on product name and category.\n\n    Returns a string of occasion keywords for BM25 search + vector encoding.\n    Keywords match the chatbot context_map in engine.py:\n      casual, office/professional, formal/ceremony, sport/athletic, evening/party.\n    \"\"\"\n    text = f\"{product_name} {category}\".lower()\n    occasions = []\n\n    # Sport / Athletic\n    if any(kw in text for kw in [\n        \"running\", \"trainer\", \"sneaker\", \"jogger\", \"legging\", \"sports\",\n        \"athletic\", \"gym\", \"yoga\", \"swim\", \"activewear\", \"workout\",\n        \"fitness\", \"track\", \"cycling\", \"football\", \"basketball\",\n    ]):\n        occasions.append(\"sport athletic activewear gym workout outdoor\")\n\n    # Formal / Ceremony\n    if any(kw in text for kw in [\n        \"suit\", \"tuxedo\", \"gown\", \"evening dress\", \"formal\", \"bow tie\",\n        \"cufflink\", \"waistcoat\", \"wedding\",\n    ]):\n        occasions.append(\"formal ceremony wedding elegant evening\")\n\n    # Office / Professional\n    if any(kw in text for kw in [\n        \"blazer\", \"shirt\", \"blouse\", \"trouser\", \"pencil\", \"chino\",\n        \"oxford\", \"loafer\", \"smart\", \"tailored\",\n    ]):\n        occasions.append(\"office work professional business classic\")\n\n    # Evening / Party\n    if any(kw in text for kw in [\n        \"sequin\", \"glitter\", \"satin\", \"silk\", \"club\", \"party\", \"cocktail\",\n        \"bodycon\", \"metallic\", \"sparkle\", \"mini dress\", \"evening\",\n    ]):\n        occasions.append(\"party evening nightout glamorous\")\n\n    # Outdoor / Winter\n    if any(kw in text for kw in [\n        \"coat\", \"puffer\", \"parka\", \"scarf\", \"glove\", \"beanie\",\n        \"thermal\", \"fleece\", \"waterproof\", \"rain\", \"hiking\", \"down jacket\",\n    ]):\n        occasions.append(\"outdoor winter cold weather\")\n\n    # Beach / Summer\n    if any(kw in text for kw in [\n        \"bikini\", \"swimsuit\", \"swimwear\", \"sandal\", \"flip flop\", \"linen\",\n        \"tank top\", \"vest top\", \"beach\", \"tropical\", \"resort\",\n    ]):\n        occasions.append(\"beach summer vacation holiday\")\n\n    # Casual / Everyday (default)\n    if any(kw in text for kw in [\n        \"t-shirt\", \"tee\", \"jeans\", \"denim\", \"hoodie\", \"sweatshirt\",\n        \"cardigan\", \"pullover\", \"polo\", \"shorts\", \"canvas\", \"jersey\",\n        \"crop top\", \"cargo\",\n    ]):\n        occasions.append(\"casual everyday relaxed daily\")\n\n    if not occasions:\n        occasions.append(\"casual everyday\")\n\n    return \" | \".join(occasions)\n\n\ndef build_description(item):\n    \"\"\"Build rich text from ASOS product fields for BM25 + text encoding.\"\"\"\n    parts = []\n    name = item.get(\"name\", \"\") or \"\"\n    if name:\n        parts.append(name)\n    color = item.get(\"color\", \"\") or \"\"\n    if color:\n        parts.append(f\"Color: {color}\")\n    category = item.get(\"category\", \"\") or \"\"\n    if category and category != name:\n        parts.append(f\"Category: {category}\")\n    price = item.get(\"price\", \"\") or \"\"\n    if price:\n        parts.append(f\"Price: {price}\")\n    size = item.get(\"size\", \"\") or \"\"\n    if size:\n        parts.append(f\"Sizes: {size}\")\n\n    raw_desc = item.get(\"description\", \"\")\n    if raw_desc:\n        try:\n            if isinstance(raw_desc, str):\n                desc_list = json.loads(raw_desc.replace(\"'\", '\"'))\n            else:\n                desc_list = raw_desc\n            if isinstance(desc_list, list):\n                for entry in desc_list:\n                    if isinstance(entry, dict):\n                        for key, val in entry.items():\n                            if key == \"Brand\":\n                                continue\n                            parts.append(str(val))\n        except (json.JSONDecodeError, ValueError):\n            cleaned = re.sub(r\"<[^>]+>\", \" \", str(raw_desc))\n            cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n            if cleaned:\n                parts.append(cleaned)\n\n    return \" | \".join(parts)\n\n\ndef download_image(url, timeout=10):\n    try:\n        r = requests.get(url, timeout=timeout, stream=True)\n        r.raise_for_status()\n        return Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n    except Exception:\n        return None\n\n\ndef download_images_parallel(urls, max_workers=8, timeout=10):\n    \"\"\"Download multiple images in parallel. Returns list of (index, url, image) tuples.\"\"\"\n    results = []\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        future_to_info = {\n            executor.submit(download_image, url, timeout): (idx, url)\n            for idx, url in enumerate(urls)\n        }\n        for future in as_completed(future_to_info):\n            idx, url = future_to_info[future]\n            img = future.result()\n            if img is not None:\n                results.append((idx, url, img))\n    results.sort(key=lambda x: x[0])\n    return results\n\n\ndef make_thumbnail_b64(image, size=THUMBNAIL_SIZE):\n    img = image.copy()\n    img.thumbnail((size, size))\n    if img.mode in (\"RGBA\", \"P\"):\n        img = img.convert(\"RGB\")\n    buf = io.BytesIO()\n    img.save(buf, format=\"JPEG\", quality=85)\n    return base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n\n\n# --- Single-image encode functions ---\n\ndef _to_tensor(feat):\n    \"\"\"Extract tensor from model output (handles both raw tensors and BaseModelOutput).\"\"\"\n    if isinstance(feat, torch.Tensor):\n        return feat\n    if hasattr(feat, \"pooler_output\") and feat.pooler_output is not None:\n        return feat.pooler_output\n    if hasattr(feat, \"last_hidden_state\"):\n        return feat.last_hidden_state[:, 0, :]\n    raise ValueError(f\"Cannot extract tensor from {type(feat)}\")\n\n\ndef _encode_image(model_type, model, processor, img):\n    \"\"\"Encode image -> normalized numpy vector.\"\"\"\n    if model_type == \"openclip\":\n        image_tensor = processor(img).unsqueeze(0).to(device)\n        feat = model.encode_image(image_tensor)\n    else:\n        inputs = processor(images=img, return_tensors=\"pt\")\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        feat = model.get_image_features(**inputs)\n    feat = _to_tensor(feat)\n    feat = feat / feat.norm(p=2, dim=-1, keepdim=True)\n    return feat.cpu().float().numpy().flatten()\n\n\ndef _encode_text(model_type, model, processor, tokenizer, text, max_len=77):\n    \"\"\"Encode text -> normalized numpy vector. Truncates to max_len tokens.\"\"\"\n    if model_type == \"openclip\":\n        tokens = tokenizer([text]).to(device)\n        feat = model.encode_text(tokens)\n    else:\n        tok = getattr(processor, \"tokenizer\", processor)\n        inputs = tok(\n            [text], return_tensors=\"pt\", padding=True,\n            truncation=True, max_length=max_len,\n        )\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        feat = model.get_text_features(**inputs)\n    feat = _to_tensor(feat)\n    feat = feat / feat.norm(p=2, dim=-1, keepdim=True)\n    return feat.cpu().float().numpy().flatten()\n\n\n# --- Batch encode functions (N images at once through GPU) ---\n\ndef _encode_images_batch(model_type, model, processor, images):\n    \"\"\"Encode a batch of images -> normalized numpy vectors (N, D).\"\"\"\n    if model_type == \"openclip\":\n        batch = torch.stack([processor(img) for img in images]).to(device)\n        feat = model.encode_image(batch)\n    else:\n        inputs = processor(images=images, return_tensors=\"pt\")\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        feat = model.get_image_features(**inputs)\n    feat = _to_tensor(feat)\n    feat = feat / feat.norm(p=2, dim=-1, keepdim=True)\n    return feat.cpu().float().numpy()\n\n\ndef _encode_texts_batch(model_type, model, processor, tokenizer, texts, max_len=77):\n    \"\"\"Encode a batch of texts -> normalized numpy vectors (N, D).\"\"\"\n    if model_type == \"openclip\":\n        tokens = tokenizer(texts).to(device)\n        feat = model.encode_text(tokens)\n    else:\n        tok = getattr(processor, \"tokenizer\", processor)\n        inputs = tok(\n            texts, return_tensors=\"pt\", padding=True,\n            truncation=True, max_length=max_len,\n        )\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        feat = model.get_text_features(**inputs)\n    feat = _to_tensor(feat)\n    feat = feat / feat.norm(p=2, dim=-1, keepdim=True)\n    return feat.cpu().float().numpy()\n\n\n# Pre-create one CUDA stream per model for true GPU parallelism\n_model_streams = {}\nif device == \"cuda\":\n    for name in MODELS:\n        _model_streams[name] = torch.cuda.Stream()\n    print(f\"Created {len(_model_streams)} CUDA streams for parallel encoding\")\n\n\ndef encode_combined_all(pil_img, description, image_weight=IMAGE_WEIGHT):\n    \"\"\"Encode 1 image+text avec les 3 modeles en parallele (CUDA streams).\n    Kept for backward compat / single-image use cases.\n    \"\"\"\n    img = pil_img.convert(\"RGB\")\n    has_text = bool(description and description.strip())\n\n    def encode_one(name, model_tuple):\n        model_type, model, processor, tokenizer, max_len = model_tuple\n        stream = _model_streams.get(name)\n        stream_ctx = torch.cuda.stream(stream) if stream else nullcontext()\n        with torch.no_grad(), stream_ctx:\n            img_vec = _encode_image(model_type, model, processor, img)\n            if has_text:\n                txt_vec = _encode_text(model_type, model, processor, tokenizer, description, max_len)\n                combined = image_weight * img_vec + (1 - image_weight) * txt_vec\n                norm = np.linalg.norm(combined)\n                if norm > 0:\n                    combined = combined / norm\n                result = combined\n            else:\n                result = img_vec\n        if stream:\n            stream.synchronize()\n        return name, result\n\n    with ThreadPoolExecutor(max_workers=3) as executor:\n        futures = [executor.submit(encode_one, n, t) for n, t in MODELS.items()]\n        return {f.result()[0]: f.result()[1] for f in futures}\n\n\ndef encode_combined_batch(images, texts, image_weight=IMAGE_WEIGHT):\n    \"\"\"Encode N images+texts avec les 3 modeles en parallele (batch GPU + CUDA streams).\n\n    Chaque modele traite le batch entier d'images d'un coup (GPU saturation),\n    et les 3 modeles tournent en parallele via CUDA streams + ThreadPoolExecutor.\n\n    Args:\n        images: list of PIL images\n        texts: list of text strings (same length as images)\n        image_weight: weight for image vectors (1-weight for text)\n\n    Returns:\n        list of dicts {model_name: numpy vector}\n    \"\"\"\n    n = len(images)\n    imgs = [img.convert(\"RGB\") for img in images]\n    has_texts = [bool(t and t.strip()) for t in texts]\n    text_indices = [i for i in range(n) if has_texts[i]]\n    batch_texts = [texts[i] for i in text_indices] if text_indices else []\n\n    def encode_model(name, model_tuple):\n        model_type, mdl, processor, tokenizer, max_len = model_tuple\n        stream = _model_streams.get(name)\n        stream_ctx = torch.cuda.stream(stream) if stream else nullcontext()\n        with torch.no_grad(), stream_ctx:\n            img_vecs = _encode_images_batch(model_type, mdl, processor, imgs)\n            if text_indices:\n                txt_vecs = _encode_texts_batch(model_type, mdl, processor, tokenizer, batch_texts, max_len)\n                combined = img_vecs.copy()\n                for j, idx in enumerate(text_indices):\n                    c = image_weight * img_vecs[idx] + (1 - image_weight) * txt_vecs[j]\n                    norm_val = np.linalg.norm(c)\n                    if norm_val > 0:\n                        c = c / norm_val\n                    combined[idx] = c\n                result = combined\n            else:\n                result = img_vecs\n        if stream:\n            stream.synchronize()\n        return name, result\n\n    with ThreadPoolExecutor(max_workers=3) as executor:\n        futures = [executor.submit(encode_model, nm, t) for nm, t in MODELS.items()]\n        all_vectors = {f.result()[0]: f.result()[1] for f in futures}\n\n    return [{name: all_vectors[name][i] for name in all_vectors} for i in range(n)]\n\n\n# Quick sanity check (single image)\n_test_img = Image.new(\"RGB\", (64, 64), \"red\")\n_test_vecs = encode_combined_all(_test_img, \"red test shirt\")\nfor k, v in _test_vecs.items():\n    print(f\"  {k}: shape={v.shape}, dtype={v.dtype}\")\n\n# Sanity check (batch of 4)\n_test_imgs = [Image.new(\"RGB\", (64, 64), c) for c in [\"red\", \"blue\", \"green\", \"black\"]]\n_test_texts = [\"red shirt\", \"blue jeans\", \"green jacket\", \"black shoes\"]\n_test_batch = encode_combined_batch(_test_imgs, _test_texts)\nprint(f\"  Batch: {len(_test_batch)} items, keys={list(_test_batch[0].keys())}\")\nfor k in _test_batch[0]:\n    print(f\"    {k}: shape={_test_batch[0][k].shape}\")\n\n# Test detect_occasion\nprint(\"\\nOccasion examples:\")\nprint(f\"  'running trainer' → {detect_occasion('Nike running trainer', 'shoes')}\")\nprint(f\"  'sequin dress' → {detect_occasion('ASOS sequin mini dress', 'dresses')}\")\nprint(f\"  'puffer jacket' → {detect_occasion('Puffer jacket in black', 'coats')}\")\nprint(f\"  'slim jeans' → {detect_occasion('Slim jeans in blue', 'jeans')}\")\nprint(f\"  'oxford shirt' → {detect_occasion('Oxford shirt white', 'shirts')}\")\nprint(\"OK\")"
  },
  {
   "cell_type": "code",
   "source": "## Benchmark : single-image vs batch encoding\nimport time\n\nN_BENCH = 32  # nombre d'images pour le benchmark\n_bench_imgs = [Image.new(\"RGB\", (384, 384), c) for c in [\"blue\", \"red\", \"green\", \"black\"] * (N_BENCH // 4)]\n_bench_texts = [f\"color {i} denim jacket casual style\" for i in range(N_BENCH)]\n\n# Warmup GPU\nfor _ in range(3):\n    encode_combined_all(_bench_imgs[0], _bench_texts[0])\nif device == \"cuda\":\n    torch.cuda.synchronize()\n\n# --- Single-image (1 par 1 avec CUDA streams) ---\nif device == \"cuda\":\n    torch.cuda.synchronize()\nt0 = time.perf_counter()\nfor img, txt in zip(_bench_imgs, _bench_texts):\n    encode_combined_all(img, txt)\nif device == \"cuda\":\n    torch.cuda.synchronize()\nsingle_time = time.perf_counter() - t0\n\n# --- Batch (toutes les images d'un coup) ---\nif device == \"cuda\":\n    torch.cuda.synchronize()\nt0 = time.perf_counter()\nencode_combined_batch(_bench_imgs, _bench_texts)\nif device == \"cuda\":\n    torch.cuda.synchronize()\nbatch_time = time.perf_counter() - t0\n\nprint(f\"Benchmark sur {N_BENCH} images ({device}):\")\nprint(f\"  Single-image : {single_time:.2f}s ({N_BENCH/single_time:.1f} img/s) — {single_time/N_BENCH*1000:.0f}ms/img\")\nprint(f\"  Batch (x{N_BENCH})  : {batch_time:.2f}s ({N_BENCH/batch_time:.1f} img/s) — {batch_time/N_BENCH*1000:.0f}ms/img\")\nprint(f\"  Speedup      : {single_time/batch_time:.1f}x\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Indexation (GPU batch encoding → Weaviate sur GCP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nfrom tqdm.auto import tqdm\nfrom PIL import Image\n\ncollection = client.collections.get(COLLECTION_NAME)\n\n# --- Checkpoint & reprise ---\ncheckpoint = load_checkpoint()\nstart_product = 0\nif RECREATE:\n    start_product = 0\nelif checkpoint[\"last_index\"] >= 0:\n    start_product = checkpoint[\"last_index\"] + 1\n    print(f\"Reprise depuis le produit {start_product} ({checkpoint['indexed_count']} deja indexes)\")\n\n# --- Recupere les product_ids deja indexes (dedup) ---\nexisting_ids = set()\nfor obj in collection.iterator(return_properties=[\"product_id\"]):\n    pid = obj.properties.get(\"product_id\", \"\")\n    if pid:\n        existing_ids.add(pid)\nprint(f\"Deja indexes: {len(existing_ids)} produits — on skip ceux-la\")\n\nindexed = checkpoint[\"indexed_count\"] if not RECREATE else 0\nerrors = checkpoint[\"errors\"] if not RECREATE else 0\nskipped = 0\nalready = 0\nstart_time = time.time()\n\n# --- Collect encode tasks from local disk (no network!) ---\nencode_tasks = []  # list of (filepath, meta)\n\npbar_prep = tqdm(total=len(dataset), desc=\"Preparation\", initial=start_product)\n\nfor product_idx in range(start_product, len(dataset)):\n    item = dataset[product_idx]\n    product_id = str(int(item.get(\"sku\", 0))) if item.get(\"sku\") else str(product_idx)\n\n    if product_id in existing_ids:\n        already += 1\n        pbar_prep.update(1)\n        continue\n\n    product_name = item.get(\"name\", \"\")\n    category = item.get(\"category\", \"\")\n    color = item.get(\"color\", \"\")\n    price = extract_price(item.get(\"price\"))\n    product_url = item.get(\"url\", \"\")\n\n    brand, desc_text = extract_description(item.get(\"description\"))\n    image_urls = extract_images(item.get(\"images\"))\n    gender = detect_gender(product_name or \"\", category or \"\")\n    occasion = detect_occasion(product_name or \"\", category or \"\")\n    description = build_description(item)\n\n    if not image_urls:\n        skipped += 1\n        pbar_prep.update(1)\n        continue\n\n    for idx, url in enumerate(image_urls[:MAX_IMAGES_PER_PRODUCT]):\n        filename = f\"{product_id}_{idx}.jpg\"\n        filepath = os.path.join(IMAGE_CACHE_DIR, filename)\n\n        if not os.path.exists(filepath):\n            skipped += 1\n            continue\n\n        meta = {\n            \"filename\": filename,\n            \"path\": url,\n            \"product_id\": product_id,\n            \"product_name\": product_name or \"\",\n            \"category\": category or \"\",\n            \"color\": color or \"\",\n            \"price\": price,\n            \"brand\": brand or \"\",\n            \"product_url\": product_url or \"\",\n            \"description\": description,\n            \"image_index\": idx,\n            \"gender\": gender,\n            \"occasion\": occasion,\n            \"_product_idx\": product_idx,\n        }\n        encode_tasks.append((filepath, meta))\n\n    pbar_prep.update(1)\n\npbar_prep.close()\nprint(f\"{already} produits deja indexes (skip)\")\nprint(f\"{len(encode_tasks)} images a encoder depuis le disque local\")\nprint(f\"Images manquantes (non telechargees): {skipped}\")\nprint(f\"Poids image/texte: {IMAGE_WEIGHT:.1f} / {1 - IMAGE_WEIGHT:.1f}\")\nprint(f\"Vecteur = image + (category, color, occasion)\")\nprint(f\"BM25 = color, category, description, occasion\")\nprint(f\"Config: ENCODE_BATCH={ENCODE_BATCH_SIZE}, WEAVIATE_BATCH={WEAVIATE_BATCH_SIZE}\")\n\n# --- Phase 2 : Batch encode (GPU pur) + push Weaviate ---\nweaviate_queue = []\nlast_product_idx = start_product\n\ndef flush_weaviate():\n    global weaviate_queue, indexed, last_product_idx\n    if not weaviate_queue:\n        return\n    with collection.batch.dynamic() as batch:\n        for doc in weaviate_queue:\n            vecs = doc.pop(\"_vectors\")\n            pidx = doc.pop(\"_product_idx\", 0)\n            batch.add_object(properties=doc, vector=vecs)\n            last_product_idx = max(last_product_idx, pidx)\n    indexed += len(weaviate_queue)\n    save_checkpoint(last_product_idx, indexed, errors)\n    weaviate_queue = []\n\n\npbar = tqdm(total=len(encode_tasks), desc=\"Phase 2 : Batch Encode + Push (GPU)\")\n\nfor batch_start in range(0, len(encode_tasks), ENCODE_BATCH_SIZE):\n    batch_slice = encode_tasks[batch_start:batch_start + ENCODE_BATCH_SIZE]\n\n    # Load images from local disk (instant, no network)\n    batch_images = []\n    batch_texts = []\n    batch_metas = []\n\n    for filepath, meta in batch_slice:\n        try:\n            img = Image.open(filepath).convert(\"RGB\")\n            w, h = img.size\n            meta = meta.copy()\n            meta[\"thumbnail_base64\"] = make_thumbnail_b64(img)\n            meta[\"width\"] = w\n            meta[\"height\"] = h\n            meta[\"indexed_at\"] = datetime.now(timezone.utc).isoformat()\n\n            # Vecteur = image + (category, color, occasion)\n            vec_text_parts = [\n                meta.get(\"category\", \"\"),\n                meta.get(\"color\", \"\"),\n                meta.get(\"occasion\", \"\"),\n            ]\n            vec_text = \" \".join(p for p in vec_text_parts if p).strip()\n\n            batch_images.append(img)\n            batch_texts.append(vec_text)\n            batch_metas.append(meta)\n        except Exception as e:\n            errors += 1\n            if errors <= 10:\n                tqdm.write(f\"  Load error ({filepath}): {e}\")\n\n    if not batch_images:\n        pbar.update(len(batch_slice))\n        continue\n\n    # Encode batch on GPU (3 models in parallel)\n    try:\n        batch_vectors = encode_combined_batch(batch_images, batch_texts)\n\n        for meta, vectors in zip(batch_metas, batch_vectors):\n            meta[\"_vectors\"] = {k: v.tolist() for k, v in vectors.items()}\n            weaviate_queue.append(meta)\n\n            if len(weaviate_queue) >= WEAVIATE_BATCH_SIZE:\n                flush_weaviate()\n    except Exception as e:\n        errors += len(batch_images)\n        if errors <= 10:\n            tqdm.write(f\"  Batch encode error: {e}\")\n\n    pbar.update(len(batch_slice))\n\n# Flush remaining\nflush_weaviate()\n\npbar.close()\nelapsed = time.time() - start_time\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Indexation terminee !\")\nprint(f\"Deja presents: {already}\")\nprint(f\"Nouvelles images indexees: {indexed}\")\nprint(f\"Sans image / non telechargees: {skipped}\")\nprint(f\"Erreurs: {errors}\")\nprint(f\"Temps: {elapsed:.0f}s ({indexed / max(elapsed, 1):.1f} images/sec)\")\nprint(f\"{'='*50}\")\nclear_checkpoint()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.collections.get(COLLECTION_NAME)\n",
    "stats = collection.aggregate.over_all(total_count=True)\n",
    "print(f\"Collection '{COLLECTION_NAME}': {stats.total_count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from weaviate.classes.query import MetadataQuery\nfrom IPython.display import display, HTML\n\nquery = \"black leather jacket\"\n\ncollection = client.collections.get(COLLECTION_NAME)\n\n# --- 1. Encode query with all 3 models ---\nquery_vectors = {}\nfor vec_name, (model_type, model, processor, tokenizer, max_len) in MODELS.items():\n    with torch.no_grad():\n        if model_type == \"openclip\":\n            tokens = tokenizer([query]).to(device)\n            features = model.encode_text(tokens)\n        else:\n            tok = getattr(processor, \"tokenizer\", processor)\n            inputs = tok(\n                [query], return_tensors=\"pt\", padding=True,\n                truncation=True, max_length=max_len,\n            )\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            features = model.get_text_features(**inputs)\n        features = _to_tensor(features)\n        features = features / features.norm(p=2, dim=-1, keepdim=True)\n        query_vectors[vec_name] = features.cpu().float().numpy().flatten()\n\n# --- 2. BM25 search ---\nbm25_results = collection.query.bm25(\n    query=query,\n    query_properties=[\"color\", \"category\", \"description\", \"occasion\"],\n    limit=60,\n    return_metadata=MetadataQuery(score=True),\n)\nprint(f\"BM25: {len(bm25_results.objects)} results\")\n\n# --- 3. near_vector per model ---\nvector_results = {}\nfor vec_name, vec in query_vectors.items():\n    results = collection.query.near_vector(\n        near_vector=vec.tolist(),\n        target_vector=vec_name,\n        limit=60,\n        return_metadata=MetadataQuery(distance=True),\n    )\n    vector_results[vec_name] = results\n    print(f\"{vec_name}: {len(results.objects)} results\")\n\n# --- 4. RRF Fusion (same as weaviate_client.py) ---\ndef rrf_fusion(result_lists, limit=20, k=60):\n    scores = {}\n    result_map = {}\n    for results in result_lists:\n        for rank, obj in enumerate(results):\n            key = obj.properties.get(\"product_id\") or obj.properties.get(\"filename\", \"\")\n            scores[key] = scores.get(key, 0) + 1 / (k + rank + 1)\n            if key not in result_map:\n                result_map[key] = obj\n    sorted_keys = sorted(scores, key=lambda x: scores[x], reverse=True)\n    return [(result_map[k], scores[k]) for k in sorted_keys[:limit]]\n\nall_lists = [bm25_results.objects]\nfor vec_name in query_vectors:\n    all_lists.append(vector_results[vec_name].objects)\n\nfused = rrf_fusion(all_lists, limit=20)\nprint(f\"\\nRRF Fusion: {len(all_lists)} sources → {len(fused)} results\")\n\n# --- 5. Display results ---\nhtml = f'<h3>Hybrid Search: \"{query}\" (BM25 + 3 vectors + RRF)</h3>'\nhtml += '<div style=\"display:flex; gap:10px; flex-wrap:wrap;\">'\nfor obj, rrf_score in fused[:20]:\n    p = obj.properties\n    thumb = p.get(\"thumbnail_base64\", \"\")\n    name = p.get(\"product_name\", \"\")[:45]\n    color = p.get(\"color\", \"\")\n    category = p.get(\"category\", \"\")\n    occasion = p.get(\"occasion\", \"\")[:30]\n    price = p.get(\"price\")\n    price_str = f\"£{price:.2f}\" if price else \"\"\n    html += f'''\n    <div style=\"text-align:center; width:170px; border:1px solid #eee; padding:5px; border-radius:8px;\">\n        <img src=\"data:image/jpeg;base64,{thumb}\" style=\"max-width:150px; max-height:150px;\"/>\n        <div style=\"font-size:11px; font-weight:bold;\">{name}</div>\n        <div style=\"font-size:10px; color:#666;\">{color} | {category}</div>\n        <div style=\"font-size:10px; color:#999;\">{occasion}</div>\n        <div style=\"font-size:11px; color:#333;\">{price_str} — RRF: {rrf_score:.4f}</div>\n    </div>'''\nhtml += '</div>'\ndisplay(HTML(html))\n\n# --- 6. Also show per-source results for comparison ---\nfor source_name, source_results in [(\"BM25\", bm25_results.objects[:5])] + [\n    (vn, vector_results[vn].objects[:5]) for vn in query_vectors\n]:\n    html = f'<h4>{source_name} top 5</h4><div style=\"display:flex; gap:8px; flex-wrap:wrap;\">'\n    for obj in source_results:\n        p = obj.properties\n        thumb = p.get(\"thumbnail_base64\", \"\")\n        name = p.get(\"product_name\", \"\")[:35]\n        color = p.get(\"color\", \"\")\n        html += f'''\n        <div style=\"text-align:center; width:130px;\">\n            <img src=\"data:image/jpeg;base64,{thumb}\" style=\"max-width:120px; max-height:120px;\"/>\n            <div style=\"font-size:10px;\">{name}</div>\n            <div style=\"font-size:10px; color:#888;\">{color}</div>\n        </div>'''\n    html += '</div>'\n    display(HTML(html))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "_keepalive_stop.set()  # Stop le thread keep-alive\nclient.close()\nprint(\"Connexion Weaviate fermee.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}