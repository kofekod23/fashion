{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion Search — Indexation ASOS depuis Colab (GPU)\n",
    "\n",
    "Ce notebook indexe le dataset [ASOS e-commerce](https://huggingface.co/datasets/UniqueData/asos-e-commerce-dataset) dans Weaviate en utilisant [Fashion CLIP](https://huggingface.co/patrickjohncyh/fashion-clip) sur GPU Colab.\n",
    "\n",
    "**Architecture :**\n",
    "- **Colab (GPU)** : encode les images avec Fashion CLIP → pousse les vecteurs dans Weaviate\n",
    "- **GCP (CPU)** : Weaviate + app FastAPI pour servir les recherches\n",
    "\n",
    "**Prérequis :**\n",
    "- Runtime GPU activé (Runtime → Change runtime type → T4 GPU)\n",
    "- Weaviate accessible sur ta VM GCP (port 8080 ouvert dans le firewall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q weaviate-client>=4.0 transformers torch torchvision Pillow datasets requests tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Renseigne l'IP externe de ta VM GCP où tourne Weaviate.\n",
    "\n",
    "**Important** : le port `8080` (HTTP) et `50051` (gRPC) doivent être ouverts dans le firewall GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "GCP_EXTERNAL_IP = \"\"   # ex: \"34.56.78.90\"\n",
    "\n",
    "WEAVIATE_HTTP_PORT = 8080\n",
    "WEAVIATE_GRPC_PORT = 50051\n",
    "\n",
    "COLLECTION_NAME = \"FashionCollection\"\n",
    "MODEL_NAME = \"patrickjohncyh/fashion-clip\"\n",
    "\n",
    "MAX_ITEMS = 2000              # produits à indexer (None = tout le dataset)\n",
    "MAX_IMAGES_PER_PRODUCT = 1    # images par produit\n",
    "BATCH_SIZE = 32               # taille des batches GPU\n",
    "WEAVIATE_BATCH_SIZE = 100     # taille des batches Weaviate\n",
    "THUMBNAIL_SIZE = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vérification GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\n\nif torch.cuda.is_available():\n    device = \"cuda\"\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nelse:\n    device = \"cpu\"\n    print(\"Pas de GPU, l'indexation sera lente.\")\n\nprint(f\"Device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chargement du modèle Fashion CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "print(f\"Chargement de {MODEL_NAME}...\")\n",
    "model = CLIPModel.from_pretrained(MODEL_NAME)\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "if device == \"cuda\":\n",
    "    model = model.half()  # FP16\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "VECTOR_DIM = model.config.projection_dim\n",
    "print(f\"Modele charge - dimension vecteur: {VECTOR_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Connexion a Weaviate sur GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.config import Configure, DataType, Property, VectorDistances\n",
    "\n",
    "assert GCP_EXTERNAL_IP, \"Renseigne GCP_EXTERNAL_IP dans la cellule de configuration.\"\n",
    "\n",
    "client = weaviate.connect_to_custom(\n",
    "    http_host=GCP_EXTERNAL_IP,\n",
    "    http_port=WEAVIATE_HTTP_PORT,\n",
    "    http_secure=False,\n",
    "    grpc_host=GCP_EXTERNAL_IP,\n",
    "    grpc_port=WEAVIATE_GRPC_PORT,\n",
    "    grpc_secure=False,\n",
    ")\n",
    "\n",
    "print(f\"Connecte a Weaviate sur {GCP_EXTERNAL_IP}\" if client.is_ready() else \"Echec de connexion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creation du schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprime et recree la collection\n",
    "if client.collections.exists(COLLECTION_NAME):\n",
    "    client.collections.delete(COLLECTION_NAME)\n",
    "    print(f\"Collection '{COLLECTION_NAME}' supprimee.\")\n",
    "\n",
    "client.collections.create(\n",
    "    name=COLLECTION_NAME,\n",
    "    properties=[\n",
    "        Property(name=\"filename\", data_type=DataType.TEXT),\n",
    "        Property(name=\"path\", data_type=DataType.TEXT),\n",
    "        Property(name=\"thumbnail_base64\", data_type=DataType.TEXT),\n",
    "        Property(name=\"width\", data_type=DataType.INT),\n",
    "        Property(name=\"height\", data_type=DataType.INT),\n",
    "        Property(name=\"indexed_at\", data_type=DataType.TEXT),\n",
    "        Property(name=\"product_id\", data_type=DataType.TEXT),\n",
    "        Property(name=\"product_name\", data_type=DataType.TEXT),\n",
    "        Property(name=\"category\", data_type=DataType.TEXT),\n",
    "        Property(name=\"color\", data_type=DataType.TEXT),\n",
    "        Property(name=\"size\", data_type=DataType.TEXT),\n",
    "        Property(name=\"price\", data_type=DataType.NUMBER),\n",
    "        Property(name=\"brand\", data_type=DataType.TEXT),\n",
    "        Property(name=\"product_url\", data_type=DataType.TEXT),\n",
    "        Property(name=\"description\", data_type=DataType.TEXT),\n",
    "        Property(name=\"image_index\", data_type=DataType.INT),\n",
    "        Property(name=\"gender\", data_type=DataType.TEXT),\n",
    "    ],\n",
    "    vectorizer_config=Configure.Vectorizer.none(),\n",
    "    vector_index_config=Configure.VectorIndex.hnsw(\n",
    "        distance_metric=VectorDistances.COSINE\n",
    "    ),\n",
    ")\n",
    "print(f\"Collection '{COLLECTION_NAME}' creee (dim={VECTOR_DIM}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Chargement du dataset ASOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Chargement du dataset ASOS...\")\n",
    "dataset = load_dataset(\"UniqueData/asos-e-commerce-dataset\", split=\"train\")\n",
    "print(f\"Dataset charge: {len(dataset)} produits\")\n",
    "\n",
    "if MAX_ITEMS:\n",
    "    dataset = dataset.select(range(min(MAX_ITEMS, len(dataset))))\n",
    "    print(f\"Limite a {len(dataset)} produits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import ast\nimport base64\nimport io\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom datetime import datetime, timezone\n\nimport requests\nfrom PIL import Image\n\n\ndef extract_images(images_field):\n    \"\"\"Extract image URLs (field is a string repr of a list).\"\"\"\n    if not images_field:\n        return []\n    if isinstance(images_field, str):\n        try:\n            parsed = ast.literal_eval(images_field)\n            if isinstance(parsed, list):\n                return [u for u in parsed if isinstance(u, str) and u.startswith(\"http\")]\n        except (ValueError, SyntaxError):\n            pass\n        if images_field.startswith(\"http\"):\n            return [images_field]\n    if isinstance(images_field, list):\n        return [u for u in images_field if isinstance(u, str) and u.startswith(\"http\")]\n    return []\n\n\ndef extract_description(desc_field):\n    \"\"\"Extract brand + text from description (string repr of list of dicts).\"\"\"\n    if not desc_field:\n        return None, \"\"\n    data = desc_field\n    if isinstance(data, str):\n        try:\n            data = ast.literal_eval(data)\n        except (ValueError, SyntaxError):\n            return None, str(desc_field)\n    if isinstance(data, list):\n        brand = None\n        texts = []\n        for entry in data:\n            if isinstance(entry, dict):\n                for key, val in entry.items():\n                    if \"brand\" in key.lower():\n                        brand = str(val) if val else None\n                    else:\n                        texts.append(str(val))\n        return brand, \" \".join(texts)\n    return None, str(desc_field)\n\n\ndef extract_price(price_field):\n    \"\"\"Extract price (string in the dataset, e.g. '49.99').\"\"\"\n    if price_field is None:\n        return None\n    try:\n        val = float(price_field)\n        return val if val > 0 else None\n    except (ValueError, TypeError):\n        return None\n\n\ndef detect_gender(product_name, category):\n    text = f\"{product_name} {category}\".lower()\n    for kw in [\"women's\", \"womens\", \"female\", \"femme\", \" woman \", \"for women\", \"ladies\", \"maternity\"]:\n        if kw in text:\n            return \"women\"\n    for kw in [\"men's\", \"mens\", \"male\", \"homme\", \" man \", \"for men\"]:\n        if kw in text:\n            return \"men\"\n    return None\n\n\ndef download_image(url, timeout=10):\n    try:\n        r = requests.get(url, timeout=timeout, stream=True)\n        r.raise_for_status()\n        return Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n    except Exception:\n        return None\n\n\ndef download_images_parallel(urls, max_workers=8, timeout=10):\n    \"\"\"Download multiple images in parallel. Returns list of (index, url, image) tuples.\"\"\"\n    results = []\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        future_to_info = {\n            executor.submit(download_image, url, timeout): (idx, url)\n            for idx, url in enumerate(urls)\n        }\n        for future in as_completed(future_to_info):\n            idx, url = future_to_info[future]\n            img = future.result()\n            if img is not None:\n                results.append((idx, url, img))\n    results.sort(key=lambda x: x[0])\n    return results\n\n\ndef make_thumbnail_b64(image, size=THUMBNAIL_SIZE):\n    img = image.copy()\n    img.thumbnail((size, size))\n    if img.mode in (\"RGBA\", \"P\"):\n        img = img.convert(\"RGB\")\n    buf = io.BytesIO()\n    img.save(buf, format=\"JPEG\", quality=85)\n    return base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n\n\ndef encode_images_batch(images):\n    \"\"\"Encode a batch of PIL images → (batch_size, 512) numpy array, L2-normalized.\"\"\"\n    with torch.no_grad():\n        inputs = processor(images=images, return_tensors=\"pt\")\n        pixel_values = inputs[\"pixel_values\"].to(device)\n        # Use vision model + projection directly (avoids return type issues)\n        vision_out = model.vision_model(pixel_values=pixel_values)\n        features = model.visual_projection(vision_out.pooler_output)\n        features = features / features.norm(p=2, dim=-1, keepdim=True)\n    return features.cpu().float().numpy()\n\n\n# Quick sanity check\n_test_img = Image.new(\"RGB\", (64, 64), \"red\")\n_test_vec = encode_images_batch([_test_img])\nprint(f\"OK — encode test: shape={_test_vec.shape}, dtype={_test_vec.dtype}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Indexation (GPU batch encoding → Weaviate sur GCP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nfrom tqdm.auto import tqdm\n\ncollection = client.collections.get(COLLECTION_NAME)\n\nimage_batch = []\nmeta_batch = []\nweaviate_queue = []\n\nindexed = 0\nskipped = 0\nstart_time = time.time()\n\n# Prefetch: collect items to download in parallel\nDOWNLOAD_WORKERS = 8\nPREFETCH_SIZE = 64  # prepare this many products' URLs at once\n\n\ndef flush_gpu_batch():\n    global image_batch, meta_batch\n    if not image_batch:\n        return\n    vectors = encode_images_batch(image_batch)\n    for vec, meta in zip(vectors, meta_batch):\n        meta[\"vector\"] = vec.tolist()\n        weaviate_queue.append(meta)\n    image_batch = []\n    meta_batch = []\n\n\ndef flush_weaviate():\n    global weaviate_queue, indexed\n    if not weaviate_queue:\n        return\n    with collection.batch.dynamic() as batch:\n        for doc in weaviate_queue:\n            vec = doc.pop(\"vector\")\n            batch.add_object(properties=doc, vector=vec)\n    indexed += len(weaviate_queue)\n    weaviate_queue = []\n\n\n# Collect all items with their metadata and URLs to download\ndownload_tasks = []  # list of (url, meta_dict, product_idx)\n\npbar_prep = tqdm(total=len(dataset), desc=\"Preparation\")\n\nfor product_idx, item in enumerate(dataset):\n    product_id = str(int(item.get(\"sku\", 0))) if item.get(\"sku\") else str(product_idx)\n    product_name = item.get(\"name\", \"\")\n    category = item.get(\"category\", \"\")\n    color = item.get(\"color\", \"\")\n    price = extract_price(item.get(\"price\"))\n    product_url = item.get(\"url\", \"\")\n\n    brand, description = extract_description(item.get(\"description\"))\n    image_urls = extract_images(item.get(\"images\"))\n    gender = detect_gender(product_name or \"\", category or \"\")\n\n    if not image_urls:\n        skipped += 1\n        pbar_prep.update(1)\n        continue\n\n    for idx, url in enumerate(image_urls[:MAX_IMAGES_PER_PRODUCT]):\n        meta = {\n            \"filename\": f\"{product_id}_{idx}.jpg\",\n            \"path\": url,\n            \"product_id\": product_id,\n            \"product_name\": product_name or \"\",\n            \"category\": category or \"\",\n            \"color\": color or \"\",\n            \"price\": price,\n            \"brand\": brand or \"\",\n            \"product_url\": product_url or \"\",\n            \"description\": description,\n            \"image_index\": idx,\n            \"gender\": gender,\n        }\n        download_tasks.append((url, meta))\n\n    pbar_prep.update(1)\n\npbar_prep.close()\nprint(f\"\\n{len(download_tasks)} images a telecharger pour {len(dataset)} produits ({skipped} sans URL)\")\n\n# Download images in parallel batches and encode on GPU\npbar = tqdm(total=len(download_tasks), desc=\"Download + Encode\")\n\nfor batch_start in range(0, len(download_tasks), PREFETCH_SIZE):\n    batch_slice = download_tasks[batch_start:batch_start + PREFETCH_SIZE]\n    urls = [t[0] for t in batch_slice]\n\n    # Parallel download\n    downloaded = download_images_parallel(urls, max_workers=DOWNLOAD_WORKERS)\n\n    for local_idx, url, img in downloaded:\n        meta = batch_slice[local_idx][1].copy()\n        w, h = img.size\n        meta[\"thumbnail_base64\"] = make_thumbnail_b64(img)\n        meta[\"width\"] = w\n        meta[\"height\"] = h\n        meta[\"indexed_at\"] = datetime.now(timezone.utc).isoformat()\n\n        image_batch.append(img)\n        meta_batch.append(meta)\n\n        if len(image_batch) >= BATCH_SIZE:\n            flush_gpu_batch()\n\n        if len(weaviate_queue) >= WEAVIATE_BATCH_SIZE:\n            flush_weaviate()\n\n    pbar.update(len(batch_slice))\n\nflush_gpu_batch()\nflush_weaviate()\n\npbar.close()\nelapsed = time.time() - start_time\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Indexation terminee !\")\nprint(f\"Images indexees: {indexed}\")\nprint(f\"Produits sans image: {skipped}\")\nprint(f\"Temps: {elapsed:.0f}s ({indexed / max(elapsed, 1):.1f} images/sec)\")\nprint(f\"{'='*50}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.collections.get(COLLECTION_NAME)\n",
    "stats = collection.aggregate.over_all(total_count=True)\n",
    "print(f\"Collection '{COLLECTION_NAME}': {stats.total_count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from weaviate.classes.query import MetadataQuery\nfrom IPython.display import display, HTML\n\nquery = \"black leather jacket\"\n\nwith torch.no_grad():\n    inputs = processor(text=[query], return_tensors=\"pt\", padding=True, truncation=True)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    text_out = model.text_model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n    features = model.text_projection(text_out.pooler_output)\n    features = features / features.norm(p=2, dim=-1, keepdim=True)\n    query_vector = features.cpu().float().numpy().flatten().tolist()\n\nresults = collection.query.near_vector(\n    near_vector=query_vector,\n    limit=5,\n    return_metadata=MetadataQuery(distance=True),\n)\n\nprint(f\"Resultats pour: '{query}'\\n\")\n\nhtml = '<div style=\"display:flex; gap:10px; flex-wrap:wrap;\">'\nfor obj in results.objects:\n    p = obj.properties\n    score = f\"{1 - (obj.metadata.distance or 0):.3f}\"\n    thumb = p.get(\"thumbnail_base64\", \"\")\n    name = p.get(\"product_name\", \"\")\n    price = p.get(\"price\")\n    price_str = f\"\\u00a3{price:.2f}\" if price else \"\"\n    html += f'''\n    <div style=\"text-align:center; width:160px;\">\n        <img src=\"data:image/jpeg;base64,{thumb}\" style=\"max-width:150px; max-height:150px;\"/>\n        <div style=\"font-size:11px;\">{name[:40]}</div>\n        <div style=\"font-size:11px; color:gray;\">{price_str} - score: {score}</div>\n    </div>'''\nhtml += '</div>'\ndisplay(HTML(html))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "print(\"Connexion Weaviate fermee.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}