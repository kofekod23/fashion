{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fashion Search — Indexation ASOS depuis Colab (GPU)\n\nCe notebook indexe le dataset [ASOS e-commerce](https://huggingface.co/datasets/UniqueData/asos-e-commerce-dataset) dans Weaviate avec **3 named vectors** :\n- **fashion_clip** (512d) — `patrickjohncyh/fashion-clip`\n- **marqo_clip** (512d) — `Marqo/marqo-fashionCLIP`\n- **siglip2** (1152d) — `google/siglip2-so400m-patch14-384`\n\n**Features :**\n- Combined image + text embeddings (0.7 img + 0.3 txt) pour des vecteurs plus riches\n- Descriptions texte pour BM25F search\n- Checkpoint / reprise automatique\n- Keep-alive Weaviate (ping toutes les 2 min)\n\n**Architecture :**\n- **Colab (GPU)** : encode les images avec les 3 modèles → pousse les vecteurs dans Weaviate\n- **GCP (CPU)** : Weaviate + app FastAPI pour servir les recherches\n\n**Prérequis :**\n- Runtime GPU activé (Runtime → Change runtime type → T4 GPU)\n- Weaviate accessible sur ta VM GCP (port 8080 ouvert dans le firewall)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install deps (sans toucher au torch pre-installe sur Colab)\n!pip install -q weaviate-client>=4.0 transformers Pillow datasets requests tqdm timm ftfy regex\n!pip install -q --no-deps open-clip-torch"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Renseigne l'IP externe de ta VM GCP où tourne Weaviate.\n",
    "\n",
    "**Important** : le port `8080` (HTTP) et `50051` (gRPC) doivent être ouverts dans le firewall GCP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- CONFIGURATION ---\nGCP_EXTERNAL_IP = \"\"   # ex: \"34.56.78.90\"\n\nWEAVIATE_HTTP_PORT = 8080\nWEAVIATE_GRPC_PORT = 50051\n\nCOLLECTION_NAME = \"FashionCollection\"\n\n# Modèles\nMODEL_FASHION_CLIP = \"patrickjohncyh/fashion-clip\"\nMODEL_MARQO_CLIP = \"Marqo/marqo-fashionCLIP\"\nMODEL_SIGLIP2 = \"google/siglip2-so400m-patch14-384\"\n\n# Named vector keys\nVECTOR_FASHION_CLIP = \"fashion_clip\"\nVECTOR_MARQO_CLIP = \"marqo_clip\"\nVECTOR_SIGLIP2 = \"siglip2\"\nALL_VECTOR_NAMES = [VECTOR_FASHION_CLIP, VECTOR_MARQO_CLIP, VECTOR_SIGLIP2]\n\n# --- Tuning A100 40GB + 83GB RAM ---\nMAX_ITEMS = None              # tout le dataset (~45k produits)\nMAX_IMAGES_PER_PRODUCT = 1    # images par produit\nWEAVIATE_BATCH_SIZE = 200     # flush frequent = meilleur checkpoint\nDOWNLOAD_WORKERS = 64         # téléchargements parallèles\nPREFETCH_SIZE = 200           # petit = barre de progrès en temps réel\nTHUMBNAIL_SIZE = 150\nIMAGE_WEIGHT = 0.7            # poids image dans le vecteur combiné\nRECREATE = False              # False = reprise, True = tout recréer\n\n# Checkpoint (reprise automatique)\nCHECKPOINT_FILE = \"/content/index_checkpoint.json\"\nKEEPALIVE_INTERVAL = 120      # secondes"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vérification GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\n\nif torch.cuda.is_available():\n    device = \"cuda\"\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nelse:\n    device = \"cpu\"\n    print(\"Pas de GPU, l'indexation sera lente.\")\n\nprint(f\"Device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Chargement des 3 modèles"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import CLIPModel, CLIPProcessor, AutoModel, AutoProcessor\nimport open_clip\n\n# Fashion CLIP (512d) — HuggingFace format\nprint(f\"Chargement de {MODEL_FASHION_CLIP}...\")\nfc_model = CLIPModel.from_pretrained(MODEL_FASHION_CLIP).to(device).eval()\nfc_processor = CLIPProcessor.from_pretrained(MODEL_FASHION_CLIP)\n\n# Marqo FashionCLIP (512d) — OpenCLIP format\nprint(f\"Chargement de {MODEL_MARQO_CLIP}...\")\nmq_model, _, mq_preprocess = open_clip.create_model_and_transforms(\n    f\"hf-hub:{MODEL_MARQO_CLIP}\", device=device\n)\nmq_tokenizer = open_clip.get_tokenizer(f\"hf-hub:{MODEL_MARQO_CLIP}\")\nmq_model = mq_model.eval()\n\n# SigLIP2-SO400M (1152d) — HuggingFace format\nprint(f\"Chargement de {MODEL_SIGLIP2}...\")\nsl_model = AutoModel.from_pretrained(MODEL_SIGLIP2).to(device).eval()\nsl_processor = AutoProcessor.from_pretrained(MODEL_SIGLIP2)\n\n# Dict unifie : (type, model, processor/preprocess, tokenizer, max_text_tokens)\nMODELS = {\n    VECTOR_FASHION_CLIP: (\"hf\", fc_model, fc_processor, None, 77),\n    VECTOR_MARQO_CLIP: (\"openclip\", mq_model, mq_preprocess, mq_tokenizer, 77),\n    VECTOR_SIGLIP2: (\"hf\", sl_model, sl_processor, None, 64),\n}\n\n# A100 40GB : ~5GB en FP32 pour les 3 modeles, pas besoin de FP16\nif device == \"cuda\":\n    vram_used = torch.cuda.memory_allocated() / 1024**3\n    vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"\\n3 modeles charges sur {device} — VRAM: {vram_used:.1f}/{vram_total:.0f} GB\")\nelse:\n    print(f\"\\n3 modeles charges sur {device}.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Connexion a Weaviate sur GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import weaviate\nimport json\nimport os\nimport threading\nfrom datetime import datetime, timezone\nfrom weaviate.classes.config import Configure, DataType, Property, VectorDistances\n\nassert GCP_EXTERNAL_IP, \"Renseigne GCP_EXTERNAL_IP dans la cellule de configuration.\"\n\nclient = weaviate.connect_to_custom(\n    http_host=GCP_EXTERNAL_IP,\n    http_port=WEAVIATE_HTTP_PORT,\n    http_secure=False,\n    grpc_host=GCP_EXTERNAL_IP,\n    grpc_port=WEAVIATE_GRPC_PORT,\n    grpc_secure=False,\n)\n\nprint(f\"Connecte a Weaviate sur {GCP_EXTERNAL_IP}\" if client.is_ready() else \"Echec de connexion\")\n\n# --- Keep-alive (ping toutes les 2 min, auto-reconnect) ---\n_keepalive_stop = threading.Event()\n\ndef start_keepalive(wv_client, interval=KEEPALIVE_INTERVAL):\n    def _run():\n        while not _keepalive_stop.wait(interval):\n            try:\n                if not wv_client.is_ready():\n                    raise ConnectionError(\"not ready\")\n            except Exception as e:\n                print(f\"\\n[keep-alive] Reconnexion... ({e})\")\n                try:\n                    wv_client.close()\n                except Exception:\n                    pass\n                wv_client.connect_to_custom(\n                    http_host=GCP_EXTERNAL_IP,\n                    http_port=WEAVIATE_HTTP_PORT,\n                    http_secure=False,\n                    grpc_host=GCP_EXTERNAL_IP,\n                    grpc_port=WEAVIATE_GRPC_PORT,\n                    grpc_secure=False,\n                )\n    t = threading.Thread(target=_run, daemon=True)\n    t.start()\n    return t\n\nstart_keepalive(client)\nprint(\"Keep-alive demarre (ping toutes les 2 min)\")\n\n# --- Checkpoint helpers ---\ndef load_checkpoint():\n    try:\n        if os.path.exists(CHECKPOINT_FILE):\n            with open(CHECKPOINT_FILE) as f:\n                return json.load(f)\n    except Exception:\n        pass\n    return {\"last_index\": -1, \"indexed_count\": 0, \"errors\": 0}\n\ndef save_checkpoint(last_index, indexed_count, errors):\n    with open(CHECKPOINT_FILE, \"w\") as f:\n        json.dump({\"last_index\": last_index, \"indexed_count\": indexed_count,\n                    \"errors\": errors, \"timestamp\": datetime.now(timezone.utc).isoformat()}, f)\n\ndef clear_checkpoint():\n    if os.path.exists(CHECKPOINT_FILE):\n        os.remove(CHECKPOINT_FILE)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creation du schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_schema(wv_client, collection_name, vector_names=None):\n    \"\"\"Crée la collection avec named vectors.\"\"\"\n    if vector_names is None:\n        vector_names = ALL_VECTOR_NAMES\n\n    named_vectors = [\n        Configure.NamedVectors.none(\n            name=name,\n            vector_index_config=Configure.VectorIndex.hnsw(\n                distance_metric=VectorDistances.COSINE\n            ),\n        )\n        for name in vector_names\n    ]\n\n    wv_client.collections.create(\n        name=collection_name,\n        properties=[\n            Property(name=\"filename\", data_type=DataType.TEXT),\n            Property(name=\"path\", data_type=DataType.TEXT),\n            Property(name=\"thumbnail_base64\", data_type=DataType.TEXT),\n            Property(name=\"width\", data_type=DataType.INT),\n            Property(name=\"height\", data_type=DataType.INT),\n            Property(name=\"indexed_at\", data_type=DataType.TEXT),\n            Property(name=\"product_id\", data_type=DataType.TEXT),\n            Property(name=\"product_name\", data_type=DataType.TEXT),\n            Property(name=\"category\", data_type=DataType.TEXT),\n            Property(name=\"color\", data_type=DataType.TEXT),\n            Property(name=\"size\", data_type=DataType.TEXT),\n            Property(name=\"price\", data_type=DataType.NUMBER),\n            Property(name=\"brand\", data_type=DataType.TEXT),\n            Property(name=\"product_url\", data_type=DataType.TEXT),\n            Property(name=\"description\", data_type=DataType.TEXT),\n            Property(name=\"image_index\", data_type=DataType.INT),\n            Property(name=\"gender\", data_type=DataType.TEXT),\n        ],\n        vectorizer_config=named_vectors,\n    )\n    print(f\"Collection '{collection_name}' creee avec vecteurs: {vector_names}\")\n\n\nif RECREATE:\n    if client.collections.exists(COLLECTION_NAME):\n        client.collections.delete(COLLECTION_NAME)\n        print(f\"Collection '{COLLECTION_NAME}' supprimee.\")\n    create_schema(client, COLLECTION_NAME)\n    clear_checkpoint()\nelif not client.collections.exists(COLLECTION_NAME):\n    create_schema(client, COLLECTION_NAME)\n    clear_checkpoint()\nelse:\n    print(f\"Collection '{COLLECTION_NAME}' existe deja, on garde les donnees.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Chargement du dataset ASOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Chargement du dataset ASOS...\")\n",
    "dataset = load_dataset(\"UniqueData/asos-e-commerce-dataset\", split=\"train\")\n",
    "print(f\"Dataset charge: {len(dataset)} produits\")\n",
    "\n",
    "if MAX_ITEMS:\n",
    "    dataset = dataset.select(range(min(MAX_ITEMS, len(dataset))))\n",
    "    print(f\"Limite a {len(dataset)} produits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import ast\nimport base64\nimport io\nimport re\nfrom contextlib import nullcontext\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nimport numpy as np\nimport requests\nfrom PIL import Image\n\n\ndef extract_images(images_field):\n    \"\"\"Extract image URLs (field is a string repr of a list).\"\"\"\n    if not images_field:\n        return []\n    if isinstance(images_field, str):\n        try:\n            parsed = ast.literal_eval(images_field)\n            if isinstance(parsed, list):\n                return [u for u in parsed if isinstance(u, str) and u.startswith(\"http\")]\n        except (ValueError, SyntaxError):\n            pass\n        if images_field.startswith(\"http\"):\n            return [images_field]\n    if isinstance(images_field, list):\n        return [u for u in images_field if isinstance(u, str) and u.startswith(\"http\")]\n    return []\n\n\ndef extract_description(desc_field):\n    \"\"\"Extract brand + text from description (string repr of list of dicts).\"\"\"\n    if not desc_field:\n        return None, \"\"\n    data = desc_field\n    if isinstance(data, str):\n        try:\n            data = ast.literal_eval(data)\n        except (ValueError, SyntaxError):\n            return None, str(desc_field)\n    if isinstance(data, list):\n        brand = None\n        texts = []\n        for entry in data:\n            if isinstance(entry, dict):\n                for key, val in entry.items():\n                    if \"brand\" in key.lower():\n                        brand = str(val) if val else None\n                    else:\n                        texts.append(str(val))\n        return brand, \" \".join(texts)\n    return None, str(desc_field)\n\n\ndef extract_price(price_field):\n    \"\"\"Extract price (string in the dataset, e.g. '49.99').\"\"\"\n    if price_field is None:\n        return None\n    try:\n        val = float(price_field)\n        return val if val > 0 else None\n    except (ValueError, TypeError):\n        return None\n\n\ndef detect_gender(product_name, category):\n    text = f\"{product_name} {category}\".lower()\n    for kw in [\"women's\", \"womens\", \"female\", \"femme\", \" woman \", \"for women\", \"ladies\", \"maternity\"]:\n        if kw in text:\n            return \"women\"\n    for kw in [\"men's\", \"mens\", \"male\", \"homme\", \" man \", \"for men\"]:\n        if kw in text:\n            return \"men\"\n    return None\n\n\ndef build_description(item):\n    \"\"\"Build rich text from ASOS product fields for BM25 + text encoding.\"\"\"\n    parts = []\n    name = item.get(\"name\", \"\") or \"\"\n    if name:\n        parts.append(name)\n    color = item.get(\"color\", \"\") or \"\"\n    if color:\n        parts.append(f\"Color: {color}\")\n    category = item.get(\"category\", \"\") or \"\"\n    if category and category != name:\n        parts.append(f\"Category: {category}\")\n    price = item.get(\"price\", \"\") or \"\"\n    if price:\n        parts.append(f\"Price: {price}\")\n    size = item.get(\"size\", \"\") or \"\"\n    if size:\n        parts.append(f\"Sizes: {size}\")\n\n    raw_desc = item.get(\"description\", \"\")\n    if raw_desc:\n        try:\n            if isinstance(raw_desc, str):\n                desc_list = json.loads(raw_desc.replace(\"'\", '\"'))\n            else:\n                desc_list = raw_desc\n            if isinstance(desc_list, list):\n                for entry in desc_list:\n                    if isinstance(entry, dict):\n                        for key, val in entry.items():\n                            if key == \"Brand\":\n                                continue\n                            parts.append(str(val))\n        except (json.JSONDecodeError, ValueError):\n            cleaned = re.sub(r\"<[^>]+>\", \" \", str(raw_desc))\n            cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n            if cleaned:\n                parts.append(cleaned)\n\n    return \" | \".join(parts)\n\n\ndef download_image(url, timeout=10):\n    try:\n        r = requests.get(url, timeout=timeout, stream=True)\n        r.raise_for_status()\n        return Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n    except Exception:\n        return None\n\n\ndef download_images_parallel(urls, max_workers=8, timeout=10):\n    \"\"\"Download multiple images in parallel. Returns list of (index, url, image) tuples.\"\"\"\n    results = []\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        future_to_info = {\n            executor.submit(download_image, url, timeout): (idx, url)\n            for idx, url in enumerate(urls)\n        }\n        for future in as_completed(future_to_info):\n            idx, url = future_to_info[future]\n            img = future.result()\n            if img is not None:\n                results.append((idx, url, img))\n    results.sort(key=lambda x: x[0])\n    return results\n\n\ndef make_thumbnail_b64(image, size=THUMBNAIL_SIZE):\n    img = image.copy()\n    img.thumbnail((size, size))\n    if img.mode in (\"RGBA\", \"P\"):\n        img = img.convert(\"RGB\")\n    buf = io.BytesIO()\n    img.save(buf, format=\"JPEG\", quality=85)\n    return base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n\n\n# --- Encode functions (HuggingFace + OpenCLIP) ---\n\ndef _to_tensor(feat):\n    \"\"\"Extract tensor from model output (handles both raw tensors and BaseModelOutput).\"\"\"\n    if isinstance(feat, torch.Tensor):\n        return feat\n    if hasattr(feat, \"pooler_output\") and feat.pooler_output is not None:\n        return feat.pooler_output\n    if hasattr(feat, \"last_hidden_state\"):\n        return feat.last_hidden_state[:, 0, :]\n    raise ValueError(f\"Cannot extract tensor from {type(feat)}\")\n\n\ndef _encode_image(model_type, model, processor, img):\n    \"\"\"Encode image -> normalized numpy vector.\"\"\"\n    if model_type == \"openclip\":\n        image_tensor = processor(img).unsqueeze(0).to(device)\n        feat = model.encode_image(image_tensor)\n    else:\n        inputs = processor(images=img, return_tensors=\"pt\")\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        feat = model.get_image_features(**inputs)\n    feat = _to_tensor(feat)\n    feat = feat / feat.norm(p=2, dim=-1, keepdim=True)\n    return feat.cpu().float().numpy().flatten()\n\n\ndef _encode_text(model_type, model, processor, tokenizer, text, max_len=77):\n    \"\"\"Encode text -> normalized numpy vector. Truncates to max_len tokens.\"\"\"\n    if model_type == \"openclip\":\n        # open_clip tokenizer auto-truncates to 77\n        tokens = tokenizer([text]).to(device)\n        feat = model.encode_text(tokens)\n    else:\n        # Use the raw tokenizer directly to guarantee max_length is respected\n        # (AutoProcessor doesn't always forward max_length properly)\n        tok = getattr(processor, \"tokenizer\", processor)\n        inputs = tok(\n            [text], return_tensors=\"pt\", padding=True,\n            truncation=True, max_length=max_len,\n        )\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        feat = model.get_text_features(**inputs)\n    feat = _to_tensor(feat)\n    feat = feat / feat.norm(p=2, dim=-1, keepdim=True)\n    return feat.cpu().float().numpy().flatten()\n\n\n# Pre-create one CUDA stream per model for true GPU parallelism\n_model_streams = {}\nif device == \"cuda\":\n    for name in MODELS:\n        _model_streams[name] = torch.cuda.Stream()\n    print(f\"Created {len(_model_streams)} CUDA streams for parallel encoding\")\n\n\ndef encode_combined_all(pil_img, description, image_weight=IMAGE_WEIGHT):\n    \"\"\"Encode image+text avec les 3 modeles en parallele (ThreadPoolExecutor + CUDA streams).\n\n    Combined = normalize(w * img_vec + (1-w) * txt_vec)\n    Falls back to image-only si description vide.\n\n    Chaque thread utilise son propre CUDA stream pour que les kernels GPU\n    s'executent vraiment en parallele (sinon le stream par defaut serialise tout).\n    \"\"\"\n    img = pil_img.convert(\"RGB\")\n    has_text = bool(description and description.strip())\n\n    def encode_one(name, model_tuple):\n        model_type, model, processor, tokenizer, max_len = model_tuple\n        stream = _model_streams.get(name)\n        stream_ctx = torch.cuda.stream(stream) if stream else nullcontext()\n        with torch.no_grad(), stream_ctx:\n            img_vec = _encode_image(model_type, model, processor, img)\n            if has_text:\n                txt_vec = _encode_text(model_type, model, processor, tokenizer, description, max_len)\n                combined = image_weight * img_vec + (1 - image_weight) * txt_vec\n                norm = np.linalg.norm(combined)\n                if norm > 0:\n                    combined = combined / norm\n                result = combined\n            else:\n                result = img_vec\n        # Wait for this stream's GPU work to finish before returning\n        if stream:\n            stream.synchronize()\n        return name, result\n\n    with ThreadPoolExecutor(max_workers=3) as executor:\n        futures = [executor.submit(encode_one, n, t) for n, t in MODELS.items()]\n        return {f.result()[0]: f.result()[1] for f in futures}\n\n\n# Quick sanity check\n_test_img = Image.new(\"RGB\", (64, 64), \"red\")\n_test_vecs = encode_combined_all(_test_img, \"red test shirt\")\nfor k, v in _test_vecs.items():\n    print(f\"  {k}: shape={v.shape}, dtype={v.dtype}\")\nprint(\"OK\")"
  },
  {
   "cell_type": "code",
   "source": "## Benchmark : sequentiel vs parallele (CUDA streams)\nimport time\n\n_bench_img = Image.new(\"RGB\", (384, 384), \"blue\")\n_bench_desc = \"blue denim jacket with silver buttons, casual style\"\nN_RUNS = 20\n\n# Warmup GPU\nfor _ in range(3):\n    encode_combined_all(_bench_img, _bench_desc)\ntorch.cuda.synchronize() if device == \"cuda\" else None\n\n# --- Sequentiel (boucle for, pas de threads) ---\ndef encode_sequential(pil_img, description, image_weight=IMAGE_WEIGHT):\n    img = pil_img.convert(\"RGB\")\n    has_text = bool(description and description.strip())\n    vectors = {}\n    with torch.no_grad():\n        for name, (model_type, model, processor, tokenizer, max_len) in MODELS.items():\n            img_vec = _encode_image(model_type, model, processor, img)\n            if has_text:\n                txt_vec = _encode_text(model_type, model, processor, tokenizer, description, max_len)\n                combined = image_weight * img_vec + (1 - image_weight) * txt_vec\n                norm = np.linalg.norm(combined)\n                if norm > 0:\n                    combined = combined / norm\n                vectors[name] = combined\n            else:\n                vectors[name] = img_vec\n    return vectors\n\nif device == \"cuda\":\n    torch.cuda.synchronize()\nt0 = time.perf_counter()\nfor _ in range(N_RUNS):\n    encode_sequential(_bench_img, _bench_desc)\nif device == \"cuda\":\n    torch.cuda.synchronize()\nseq_time = time.perf_counter() - t0\n\n# --- Parallele (ThreadPoolExecutor + CUDA streams) ---\nif device == \"cuda\":\n    torch.cuda.synchronize()\nt0 = time.perf_counter()\nfor _ in range(N_RUNS):\n    encode_combined_all(_bench_img, _bench_desc)\nif device == \"cuda\":\n    torch.cuda.synchronize()\npar_time = time.perf_counter() - t0\n\nprint(f\"Benchmark sur {N_RUNS} images ({device}):\")\nprint(f\"  Sequentiel : {seq_time:.2f}s ({N_RUNS/seq_time:.1f} img/s) — {seq_time/N_RUNS*1000:.0f}ms/img\")\nprint(f\"  Parallele  : {par_time:.2f}s ({N_RUNS/par_time:.1f} img/s) — {par_time/N_RUNS*1000:.0f}ms/img\")\nprint(f\"  Speedup    : {seq_time/par_time:.2f}x\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Indexation (GPU batch encoding → Weaviate sur GCP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\nfrom tqdm.auto import tqdm\n\ncollection = client.collections.get(COLLECTION_NAME)\n\n# --- Checkpoint & reprise ---\ncheckpoint = load_checkpoint()\nstart_product = 0\nif RECREATE:\n    start_product = 0\nelif checkpoint[\"last_index\"] >= 0:\n    start_product = checkpoint[\"last_index\"] + 1\n    print(f\"Reprise depuis le produit {start_product} ({checkpoint['indexed_count']} deja indexes)\")\n\n# --- Recupere les product_ids deja indexes (dedup) ---\nexisting_ids = set()\nfor obj in collection.iterator(return_properties=[\"product_id\"]):\n    pid = obj.properties.get(\"product_id\", \"\")\n    if pid:\n        existing_ids.add(pid)\nprint(f\"Deja indexes: {len(existing_ids)} produits — on skip ceux-la\")\n\nindexed = checkpoint[\"indexed_count\"] if not RECREATE else 0\nerrors = checkpoint[\"errors\"] if not RECREATE else 0\nskipped = 0\nalready = 0\nstart_time = time.time()\n\n# --- Collect download tasks (skip already indexed) ---\ndownload_tasks = []\n\npbar_prep = tqdm(total=len(dataset), desc=\"Preparation\", initial=start_product)\n\nfor product_idx in range(start_product, len(dataset)):\n    item = dataset[product_idx]\n    product_id = str(int(item.get(\"sku\", 0))) if item.get(\"sku\") else str(product_idx)\n\n    if product_id in existing_ids:\n        already += 1\n        pbar_prep.update(1)\n        continue\n\n    product_name = item.get(\"name\", \"\")\n    category = item.get(\"category\", \"\")\n    color = item.get(\"color\", \"\")\n    price = extract_price(item.get(\"price\"))\n    product_url = item.get(\"url\", \"\")\n\n    brand, desc_text = extract_description(item.get(\"description\"))\n    image_urls = extract_images(item.get(\"images\"))\n    gender = detect_gender(product_name or \"\", category or \"\")\n    description = build_description(item)\n\n    if not image_urls:\n        skipped += 1\n        pbar_prep.update(1)\n        continue\n\n    for idx, url in enumerate(image_urls[:MAX_IMAGES_PER_PRODUCT]):\n        meta = {\n            \"filename\": f\"{product_id}_{idx}.jpg\",\n            \"path\": url,\n            \"product_id\": product_id,\n            \"product_name\": product_name or \"\",\n            \"category\": category or \"\",\n            \"color\": color or \"\",\n            \"price\": price,\n            \"brand\": brand or \"\",\n            \"product_url\": product_url or \"\",\n            \"description\": description,\n            \"image_index\": idx,\n            \"gender\": gender,\n            \"_product_idx\": product_idx,\n        }\n        download_tasks.append((url, meta))\n\n    pbar_prep.update(1)\n\npbar_prep.close()\nprint(f\"{already} produits deja indexes (skip)\")\nprint(f\"{len(download_tasks)} images restantes a telecharger\")\nprint(f\"Poids image/texte: {IMAGE_WEIGHT:.1f} / {1 - IMAGE_WEIGHT:.1f}\")\nprint(f\"Config: WEAVIATE_BATCH={WEAVIATE_BATCH_SIZE}, WORKERS={DOWNLOAD_WORKERS}, PREFETCH={PREFETCH_SIZE}\")\n\n# --- Download + encode (3 modeles) + push ---\nweaviate_queue = []\nlast_product_idx = start_product\n\ndef flush_weaviate():\n    global weaviate_queue, indexed, last_product_idx\n    if not weaviate_queue:\n        return\n    with collection.batch.dynamic() as batch:\n        for doc in weaviate_queue:\n            vecs = doc.pop(\"_vectors\")\n            pidx = doc.pop(\"_product_idx\", 0)\n            batch.add_object(properties=doc, vector=vecs)\n            last_product_idx = max(last_product_idx, pidx)\n    indexed += len(weaviate_queue)\n    save_checkpoint(last_product_idx, indexed, errors)\n    weaviate_queue = []\n\n\npbar = tqdm(total=len(download_tasks), desc=\"Download + Encode (3 modeles)\")\n\nfor batch_start in range(0, len(download_tasks), PREFETCH_SIZE):\n    batch_slice = download_tasks[batch_start:batch_start + PREFETCH_SIZE]\n    urls = [t[0] for t in batch_slice]\n\n    downloaded = download_images_parallel(urls, max_workers=DOWNLOAD_WORKERS)\n\n    for local_idx, url, img in downloaded:\n        try:\n            meta = batch_slice[local_idx][1].copy()\n            w, h = img.size\n            meta[\"thumbnail_base64\"] = make_thumbnail_b64(img)\n            meta[\"width\"] = w\n            meta[\"height\"] = h\n            meta[\"indexed_at\"] = datetime.now(timezone.utc).isoformat()\n\n            # Encode avec les 3 modeles (image + text combined)\n            vectors = encode_combined_all(img, meta.get(\"description\", \"\"))\n\n            meta[\"_vectors\"] = {k: v.tolist() for k, v in vectors.items()}\n            weaviate_queue.append(meta)\n\n            if len(weaviate_queue) >= WEAVIATE_BATCH_SIZE:\n                flush_weaviate()\n\n        except Exception as e:\n            errors += 1\n            if errors <= 10:\n                tqdm.write(f\"  Erreur: {e}\")\n\n    pbar.update(len(batch_slice))\n\n# Flush remaining\nflush_weaviate()\n\npbar.close()\nelapsed = time.time() - start_time\n\nprint(f\"\\n{'='*50}\")\nprint(f\"Indexation terminee !\")\nprint(f\"Deja presents: {already}\")\nprint(f\"Nouvelles images indexees: {indexed}\")\nprint(f\"Sans image: {skipped}\")\nprint(f\"Erreurs: {errors}\")\nprint(f\"Temps: {elapsed:.0f}s ({indexed / max(elapsed, 1):.1f} images/sec)\")\nprint(f\"{'='*50}\")\nclear_checkpoint()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.collections.get(COLLECTION_NAME)\n",
    "stats = collection.aggregate.over_all(total_count=True)\n",
    "print(f\"Collection '{COLLECTION_NAME}': {stats.total_count} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from weaviate.classes.query import MetadataQuery\nfrom IPython.display import display, HTML\n\nquery = \"black leather jacket\"\n\n# Test avec chaque vecteur\nfor vec_name, (model_type, model, processor, tokenizer, max_len) in MODELS.items():\n    with torch.no_grad():\n        if model_type == \"openclip\":\n            tokens = tokenizer([query]).to(device)\n            features = model.encode_text(tokens)\n        else:\n            tok = getattr(processor, \"tokenizer\", processor)\n            inputs = tok(\n                [query], return_tensors=\"pt\", padding=True,\n                truncation=True, max_length=max_len,\n            )\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            features = model.get_text_features(**inputs)\n        features = _to_tensor(features)\n        features = features / features.norm(p=2, dim=-1, keepdim=True)\n        query_vector = features.cpu().float().numpy().flatten().tolist()\n\n    results = collection.query.near_vector(\n        near_vector=query_vector,\n        target_vector=vec_name,\n        limit=5,\n        return_metadata=MetadataQuery(distance=True),\n    )\n\n    print(f\"\\n--- {vec_name} : '{query}' ---\")\n    html = '<div style=\"display:flex; gap:10px; flex-wrap:wrap;\">'\n    for obj in results.objects:\n        p = obj.properties\n        score = f\"{1 - (obj.metadata.distance or 0):.3f}\"\n        thumb = p.get(\"thumbnail_base64\", \"\")\n        name = p.get(\"product_name\", \"\")\n        price = p.get(\"price\")\n        price_str = f\"\\u00a3{price:.2f}\" if price else \"\"\n        html += f'''\n        <div style=\"text-align:center; width:160px;\">\n            <img src=\"data:image/jpeg;base64,{thumb}\" style=\"max-width:150px; max-height:150px;\"/>\n            <div style=\"font-size:11px;\">{name[:40]}</div>\n            <div style=\"font-size:11px; color:gray;\">{price_str} - score: {score}</div>\n        </div>'''\n    html += '</div>'\n    display(HTML(html))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "_keepalive_stop.set()  # Stop le thread keep-alive\nclient.close()\nprint(\"Connexion Weaviate fermee.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}